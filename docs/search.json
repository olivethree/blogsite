[
  {
    "objectID": "posts/p_1/index.html",
    "href": "posts/p_1/index.html",
    "title": "Hi, my name is…",
    "section": "",
    "text": "I love coffee and would ideally invite you to join me for a cup or two. To compensate, here’s a picture of a morning coffee scene to get you in the mood to read the rest of the monologue:\n\n\n\nRetrieved from: pexels.com\n\n\nIn 2019, I earned my PhD in Social Psychology from ISPA - Instituto Universitario. During my PhD I divided my time between Portugal and the Netherlands, conducting research at William James Center for Research and Utrecht University.\n\n\n\nMoments after my PhD defense 😄 - 16 December 2019\n\n\nMy doctoral work resulted in contributions to the fields of social perception and psychophysical image classification methods like reverse correlation.\nAfter obtaining my PhD, I moved into an industry job in pursuit of research with a more direct societal impact. I like to believe that goal was achieved during the time I spent at a Philips venture working on the development of automated clinical decision support systems. During this time my interest in the topic of human technology interaction awakened and I took an interest in roles where I could deepen my knowledge about the topic, and perhaps contribute to the domain in some way.\nI found my first opportunity to pursue the new interest in a postdoc position at Utrecht University focusing on aspects of trust in AI and human-AI interaction in general. During this period I faced some challenges (naturally): adjusting back to the academic environment, catching up with a rapidly growing field of interdisciplinary research outside of the comfort zone (social cognition), and learning how to effectively split my time between research and teaching. At the same time, I learned much about how academics approach the topic of human-AI interaction, learned much about the relevance of Psychology knowledge to the impact of AI in society, and more importantly, met wonderful people including students and colleagues with whom I learned, and keep learning, from.\nThe emergence of ChatGPT by the end of 2022 had a strong impact on the research I was conducting, raising new questions and opening up new opportunities to do research. But arguably more important was the impact of generative AI chatbots on teaching practices, which prompted us all (or should have!) to start discussing the future of Education in a world increasingly infused with tools capable of mimicking human output. The impact of AI in Education sounded like a meaningful question to invest my time in. Currently, I am doing a second postdoc at Eindhoven University of Technology. There, I spend most of my time thinking about the impact of AI in Education together with an interdisciplinary team of researchers (Economics, Philosophy, Education), university staff and students.\nIn my free time, I enjoy playing music, hunting for specialty coffee, modern and retro games, catch up with my to-read list and travelling."
  },
  {
    "objectID": "posts/p_2/index.html",
    "href": "posts/p_2/index.html",
    "title": "Using Reverse Correlation Methods in Psychology Research: R tutorial and Recommended Practices",
    "section": "",
    "text": "Last updated: : 18 August, 2024\n\n\n\nIf you found this post, chances are you might already be familiarized with the reversed correlation (RC) methodology in psychological research. But in case you never heard about it, are just curious, or need to refresh your memory, I provide a brief overview of the method below, along with R code you can use to implement the method from the start.\nThe main motivation behind this post is to offer an up-to-date tutorial on how to implement the two-phase variant of the RC methodology following the latest recommended practices. While doing so, I will show you the steps you can take to avoid a well-known limitation of this method.\n\n\nBelow I describe the steps typically involved in a two-phase reverse correlation methodology in the domain of social psychological research (see for example, Dotsch & Todorov, 2012).\nThis methodology has been often used to examine research question social judgments based on facial appearance, such as ‘Which facial cues predict that a person will be judged as having Portuguese nationality?’ or ‘Which facial cues predict that a person will be judged as trustworthy?’. Naturally, you could ask the same question about things other than faces, such as what features of an object predict that we judge it as a ‘chair’. However, for the sake of simplicity, the current overview will focus on the example of social judgments from facial appearance.\nResearch question: Imagine you’re interested in visualizing the facial cues that people use to judge someone as a trustworthy person.\n\n\n\nThe image below illustrates the 3 main steps of Phase I.\n\n\n\nThe first thing you need to implement the method is to find an image of a face. This is called the base image. There are many variables you may want to take into consideration during the selection process (for details see Brinkman et al., 2017), but picking an image of a face is what you’d go for here. This can be seen as a trick to approximate the image to what we think is the ‘prototypical’ image of the thing we want to study, which in our example is a face conveying trustworthiness. For this post, I used a randomly generated deep fake image of a female face (using thispersondoesnotexist.com). You can copy-paste it from below:\n\nTo implement the method you can use the rcicr R package (Dotsch, 2016), along with a couple of other useful R packages for data and image processing. In the R tutorial below I show you how you can setup an R project, prepare the base image to use with rcicr, generate task stimuli, and generate classification images (CIs) for different analytical scenarios (older approach vs. latest recommendations).\nRequirements: This tutorial assumes you have R and RStudio installed (R version used in this tutorial is 4.2.0, but I expect later versions to work just as well).\n\n\nI highly recommend you create a folder in your computer called “Rworkspace” or any other name you prefer, where you can store all your R projects inside. This makes it easier to organize and find your multiple projects and contents.\nNext, open RStudio, go to File > New Project\n\nNow you can select New Directory > New Project\nIn the window below, use Browse to find your “Rworkspace” folder. Once you set it, you can now enter the name of your R project under Directory name. I named my project “rc_tutorial”. This will create a folder named after the project name inside the parent folder Rworkspace. Inside your R project folder you will find a .Rproj file. This .Rproj file teels R that the root directory of your project is where it is located.\nFor now, you can simply proceed without ticking any of the options as shown below (unless you are familiarised with git version control or want to have better control of your R environment).\nClick Create Project and you’re ready to start coding!\n\nTo start coding open a R script (File > New File) and save it.\nTo install packages you can use the following function in the RStudio console:\n\ninstall.packages(\"PACKAGE_NAME_HERE\")\n\nIf you have a recent version of the RStudio it will automatically detect the packages that are required by a script and will prompt you to install them. You should start by loading all the required packages at the beginning of a script. You’ll need these:\n\n# Required packages for this tutorial\nlibrary(here) # for easy path referencing\nlibrary(tidyverse) # my preference for data processing, improved code readability\nlibrary(magrittr) # to pipe code\nlibrary(devtools) # required to load github based libraries\nlibrary(imager) # to process and edit images\n\n# Automatically detect if rcicr package (v1.0.0 -- github version) is installed. \n# If yes, loads it. Otherwise installs it and loads it.  \nif(!require(rcicr) | packageVersion(\"rcicr\") != \"1.0.0\") {\n  devtools::install_github('rdotsch/rcicr') \n  library(rcicr)\n}\n\nThe rcicr package requires you to convert the base image to grayscale, and to resize it to identical width and height (e.g. 512 by 512 pixels).\n\n# Path to image file ------------------------------------------------------\n\n# if image is in R project root directory (where Rproj file is located)\n# simply enter the filename as a string\nimg_path <- \"generated-image-bxx2g8.jpg\" \n\n# otherwise you can use the here() package to specify the path to the file\n# starting from the R project root directory\n# in the commented example below, the file would be stored in a folder named 'subfolder_X' located in the root directory\n\n# img_path <- here::here(\"subfolder_X\", \"generated-image-bxx2g8.jpg\") \n\n# Image dimensions --------------------------------------------------------\n\n# Set width in pixels\nimg_width <- 512\n\n# Automatically set height to match width (required for rcicr) \nimg_height <- img_width\n\n\n# Convert to grayscale and resize image -----------------------------------\n\nbase_image <- load.image(\"generated-image-bxx2g8.jpg\") %>% \n  imager::grayscale() %>% \n  imager::resize(size_x = img_width, size_y = img_height)\n\n\n# Save image --------------------------------------------------------------\n\nsave.image(im = base_image, \"gray_base_image.jpg\")\n\n\n\n\n\nNow you can use the base image to generate many pairs of images. These pairs of images are the stimuli you need for the 2-image forced choice task. In each pair, one image is the base image with superimposed visual noise, and the other image is the base face with the inverted noise from the first image (its negative image). The use of noise + inverted noise is a trick to maximize the differences between the images in any given task trial. This minimizes the difficulty of having to choose between highly similar images. For this example, I’m generating 500 image pairs, for a task with 500 decision trials. Here’s how you can generate them:\n\n# Load base image file ----------------------------------------------------\n\nbase_face_file <- list(\"base_face\" = \"gray_base_image.jpg\")\n\n\n# Generate image pairs ----------------------------------------------------\n\n# How many trials do you wish to have in your task?\nnumber_task_trials <- 500\n\n# Which type of visual noise do you want to generate?\n# Default is sinusoid; other options include Gabor noise  \nselected_noise <- \"sinusoid\" # or \"gabor\"\n\n# Generate stimuli\ngenerateStimuli2IFC(base_face_file, \n                    n_trials = number_task_trials,\n                    img_size = 512,\n                    noise_type = selected_noise,\n                    stimulus_path = \"./stimuli\")\n\n# WHERE ARE STIMULI STORED?\n\n## The generated stimuli can be found in a folder named 'stimuli' (or name you\n## used in stimulus_path argument above)\n\n## In this folder you can also find the .Rdata file file needed for analysis of\n## data after data collection. This .Rdata file contains the parameters that \n## were used to generate each stimulus\n\n\n\n\nNow that you have your stimuli, you can use them in a 2-image forced choice task — as we have only two images per trial, but there are other variants of the task with more images per trial (see Schmitz et al., 2021). Please note that each image pair is composed of an original image and its inverted version (its visual negative).\nUse your preferred experiment-building program to run the task. Some popular choices include Inquisit, PsychoPy, OpenSesame, Gorilla, or Qualtrics. In the image below you can see an example of a 2-image forced choice task. I highlight the essential components and the expected content of the data output, but please add Informed Consent in the beginning, and demographics (+ debriefing if necessary) at the end.\n\nImportant to take into account while building the task:\n\nImage pair presentation. Each image pair generated in Step 2 includes an image labeled as original (e.g. rcic_base_face_1_00001_ori.png) and another labeled as inverted (e.g. rcic_base_face_1_00001_inv.png). These should be presented as a pair in the same task trial. Therefore, each trial number will be associated with a specific pair of stimulus images.\nCounterbalance ori and inv images. You may want to counterbalance the position where the ori and inv images are presented, between-participants. That is, for half the participants the ori images are displayed at the left (inv at the right), and for the other half the ori images are displayed at the right (inv at the left).\nRequired data output. Your program needs to log the following information: condition of the target construct you’re interested in (e.g. trustworthy), unique subject ID, trial number, image presented on the left, image presented on the right, image selected, whether image is ori or inv, response (derived from the type of image selected (ori = +1, inv = -1). See the example data output above. The most important data to have in the end are: subject, trial number, and response coded as 1 or -1.\n\nFor the sake of this tutorial, I generated some bogus response data to use in the next steps. You can also generate it using the code below.\n\n# Simulate RC response data -----------------------------------------------\n\n# How many participants?\nnr_subjects <- 50\n# How many trials?\nnr_trials <- 500\n\n# Generate simulated data frame\nsim_data <- data.frame(\n  condition = \"TRUSTWORTHY\",\n  subject = rep(1:nr_subjects, each = nr_trials),\n  trial = rep(1:nr_trials, times = nr_subjects),\n  resp = rbinom(nr_subjects*nr_trials, 1, 0.3)\n) %>% \n  mutate(resp = ifelse(resp == 0, -1, 1))\n\n# Save data\nwrite_csv(sim_data, file = \"sim_rc_data.csv\")\n\n\n\nOnce you have finished collecting and processing your data, you are ready to generate classification images (CIs). You have several options:\n\nCreate a CI for each participant: we call this an Individual CI\nCreate a CI using the data from all participants of the same target construct condition (e.g. TRUSTWORTHY): we call this a Group CI\nCreate several CIs using randomly sampled subsets of participants from the same target construct condition (e.g. TRUSTWORTHY) we call this a Subgroup CI\n\nUntil recently, most studies using the two-phase variant of RC methods in Social Psychology, focused on generating and analyzing group CIs, as these capture the visual features predicting the target construct (TRUSTWORTHY) on which all participants of a condition agree on average.\n\nFor now, let’s focus on the traditional options of CI generation and how to do it in R.\n\n\n\n\n# Setup -------------------------------------------------------------------\n\n# Load response data\nrc_data <- vroom::vroom(\"sim_rc_data.csv\") # replace with your data filename if you're not using the tutorial bogus data\n\n# Convert data to data.frame format to prevent issues with rcicr package\nrc_data <- as.data.frame(rc_data)\n\n# Load Rdata file with stimulus generation info\n# File is stored inside the task stimuli folder when stimuli are generated\nrdata_file_location <- here::here(\"stimuli\", \"rcic_seed_1_time_Jun_10_2023_18_47.Rdata\") \n\n# Label given for base face at stimulus generation, in our case it was 'base_face'\nbase_face_label <- \"base_face\"\n\n\n# Generate Individual CIs for all participants ---------------------------------------------\n\nind_cis <- batchGenerateCI(\n  data = rc_data,\n  by = \"subject\",\n  stimuli = \"trial\",\n  responses = \"resp\",\n  baseimage = base_face_label,\n  label = \"ind_ci_\",\n  rdata = rdata_file_location,\n  targetpath = \"./individual_cis\"\n)\n\n# You will find all the individual CI image inside the folder 'individual_cis'\n\n\n# Generate Individual CI for a single participant ------------------------------------------------------\n\n# Here I'm subsetting the data from participant nr. 25\ns25_df <- rc_data %>% filter(subject == 25)\n\n# To customize the filenames of output individual CI\nparticipant_nr <- 25\n\n# Generate individual CI\nindci_25 <- generateCI(\n  stimuli = s25_df$trial,\n  responses = s25_df$resp,\n  baseimage = base_face_label,\n  filename = paste0(\"indci_\", participant_nr),\n  rdata = rdata_file_location,\n  targetpath = \"./individual_cis\"\n)\n\n# You will find the individual CI image inside the folder 'individual_cis'\n\n\n\n\nFor the tutorial example condition ‘TRUSTWORTHY’:\n\n# Setup -------------------------------------------------------------------\n\n# Load response data\nrc_data <- vroom::vroom(\"sim_rc_data.csv\") # replace with your data filename if you're not using the tutorial bogus data\n\n# Convert data to data.frame format to prevent issues with rcicr package\nrc_data <- as.data.frame(rc_data)\n\n# Load Rdata file with stimulus generation info\n# File is stored inside the task stimuli folder when stimuli are generated\nrdata_file_location <- here::here(\"stimuli\", \"rcic_seed_1_time_Jun_10_2023_18_47.Rdata\") \n\n# Load rdata into environment\nbase::load(rdata_file_location)\n\n# Label given for base face at stimulus generation, in our case it was 'base_face'\nbase_face_label <- \"base_face\"\n\n\n# Generate group CI for Trustworthy --------------------------------------------------\n\ngroup_cis <- batchGenerateCI(\n  data = rc_data,\n  by = \"condition\",\n  stimuli = \"trial\",\n  responses = \"resp\",\n  baseimage = base_face_label,\n  rdata = rdata_file_location,\n  targetpath = \"./group_cis\"\n)\n\n# You will find the individual CI image inside the folder 'group_cis'\n\nTraditionally, once you have the group CI for your different target conditions (e.g. trustworthy, untrustworthy, happy, sad) you can move to Phase II.\n\n\n\n\n\nIn this phase, you are usually interested in understanding if the CIs you generated in Phase I contain the signal or pattern of features that are predictive of the judgments you are studying (i.e., trustworthy, etc.). Although you can visually inspect the CIs and reach your own conclusions about whether they communicate the signal you are interested in, this would not be very reliable, as you are more biased than you think (confirmation bias…look it up!). However, you could ask an entirely new group of people to make judgments about your fresh CIs. Importantly, and ideally, these people should not have participated in the RC task in Phase I.\nTraditionally, researchers focused on collecting ratings for the group CIs of their conditions. If a group CI was generated for the concept of a ‘trustworthy’ face, then the new group of participants would evaluate how trustworthy this group CI face looks to them. If the new group of participants indeed rates this group CI as trustworthy, researchers can pop a bottle of champagne and report to the world that the group CI captured the visual cues associated with judging a face as ‘trustworthy’.\n\nAll good right?\nWell…\n\n\nA group of psychology researchers (Cone et al., 2021) called attention to the fact that evaluating a single group CI in the second phase is not ideal, as a single group CI is not able to convey any of the variability of people’s results in the first phase. This is important to consider since different groups of participants will generate slightly different group CIs for the same construct, and therefore, presenting the group of raters in the second phase with a single group CI is like presenting a randomly drawn card from the deck without showing them how the cards can vary.\nIn phase one, there is an individual result — or individual CI — for each participant. The group CI created by aggregating all the individual CIs from a given condition (for instance, ‘Trustworthy’ face) averages out any of the individual differences present in the individual CIs (how each participant imagines a “trustworthy” face to look like), keeping only the visual information that is common across the individual CIs.\nWhile there is nothing fundamentally wrong with asking a second group of participants to judge a single group CI on ‘trustworthiness’, there is an unfortunate loss of information regarding how this single image could vary depending on the visual cues that different individuals or subgroups of them (they are all sampled from a larger population after all) could have used to make their decisions in the RC task. In this way, the ratings in Phase II can capture what was common across all the individual CIs, but cannot pick up on any of the variations that occurred in peoples’ responses during Phase I.\n\n\n\nTaken to the extreme, the solution could also involve asking the CI raters in phase two to rate all of the individual CIs, thus preventing any loss of variability. However, when you have dozens if not hundreds of individual CIs, your study becomes very expensive as participants are usually paid according to the duration of the study. Plus, the task becomes increasingly tiresome for the participants who might then fall into the temptation of randomly responding.\n\nAnother recently proposed solution involves reducing the number of CIs associated with the same condition while keeping a decent amount of them. We call these subgroup CIs. This solution represents a compromise between the costly option of using all individual CIs and the problematic option of using a single group CI. The creation of subgroup CIs involves taking random subsets of participants from the same condition and generating a (sub)group CI for each of these subsets. This increases the scientific soundness and reliability of the results of Phase II, while saving us time and financial resources.\n\nIn sum, it is currently recommended that you use either all individual CIs, or subgroup CIs in Phase II.\n\nFor more detailed and formal explanations see Cone et al. (2021).\n\n\n\nThere are a couple of decisions you have to make before you start generating subgroup CIs for Phase II, including:\n\nHow many subgroup CIs you want? The maximum you can have in practice = number of all participants in condition - 1. You will want less than that I suppose (otherwise, what’s the point?).\nWhat percentage of participant IDs do you want to randomly draw from the full set for each subgroup CI? I recommend using either: 0.25, 0.50, or 0.75. Note that 1.00 (100%) is equivalent to requesting a group CI, as all participants from the condition will be used.\n\n\n# Setup -------------------------------------------------------------------\n\n# Load response data\nrc_data <- vroom::vroom(\"sim_rc_data.csv\") # replace with your data filename if you're not using the tutorial bogus data\n\n# Convert data to data.frame format to prevent issues with rcicr package\nrc_data <- as.data.frame(rc_data)\n\n# Load Rdata file with stimulus generation info\n# File is stored inside the task stimuli folder when stimuli are generated\nrdata_file_location <- here::here(\"stimuli\", \"rcic_seed_1_time_Jun_10_2023_18_47.Rdata\") \n\n# Load rdata into environment\nbase::load(rdata_file_location)\n\n# Label given for base face at stimulus generation, in our case it was 'base_face'\nbase_face_label <- \"base_face\"\n\n\n# Subgroup CI generation --------------------------------------------------\n\n# Set seed for reproducible results (default = 42, geeks will know the 'meaning' of this)\ncustom_seed <- 42\nset.seed(custom_seed)\n\n# Derive total number of participants\nparticipant_n <- rc_data$subject %>% unique %>% length\n\n# How many possible individual CIs\ntotal_ind_cis <- participant_n # This step is explicitly added for the sake of clarity\n\n# Desired number of sub group-CIs (using 4 for this tutorial)\nnr_groupcis <- 4\n\n# Fraction of participants (individual CIs) to randmoly sample from the full set\n# for each subgroup CI\n# I recommend using 0.25, 0.50, or 0.75. \n# Important: using 1.00 would be equivalent to a group CI.\ninput_fraction <- 0.50\n\n# Initialize dataframe to log information about sampled individual CIs\nsampled_cis <- NULL\n\n# Loop over \nfor(i in 1:nr_groupcis) {\n  \n  # Sampling (without replacement)\n  sampled_ind_cis <- rc_data$subject %>%\n    unique() %>%\n    sample(size = round(input_fraction * participant_n, 0),\n           replace = FALSE)\n  \n  # Filter input response data set to include only subset of participants (individual CIs)\n  subgroup_df <-\n    rc_data %>% filter(subject %in% sampled_ind_cis) %>% mutate(subgroup_ci_nr = i)\n  \n  # Subgroup CI name\n  subgroupci_label <- paste0(\"subgroup_ci_\", i)\n  \n  # Log sampled data\n  sampled_cis <- rbind(sampled_cis, data.frame(subgroupci_label, sampled_ind_cis))\n  \n  \n  # Generate subgroup CI\n  sg_ci <- batchGenerateCI(\n    subgroup_df,\n    by = \"subgroup_ci_nr\",\n    stimuli = \"trial\",\n    responses = \"resp\",\n    baseimage = base_face_label,\n    rdata = rdata_file_location,\n    save_as_png = TRUE,\n    targetpath = \"./subgroup_cis\",\n    antiCI = FALSE,\n    scaling = \"autoscale\",\n    constant = 0.1,\n    label = subgroupci_label\n  )\n}\n\n# You can find the subgroup CIs inside the folder 'subgroup_cis'\n\n# Save data file with onformation about which participant Ids were sampled for each subgroupCI\nwrite_csv(sampled_cis, file = \"pids_subgroupcis_data.csv\")\n\n\n\nAlternatively, when you already have the Phase I task response data and want to quickly generate a dataset that you can use to generate subgroup CIs, you can bypass some of the initial coding and use the shiny app below. However, the main benefit of using this app is to speed things up, and you still need to go back to R coding after getting the output from it.\n🤔 Currently wondering if this app is really that useful, since most of you will be good at coding if you got into RC methods…\nShiny app: https://github.com/olivethree/shinyrc_subgroupcis\n\n\n\n\n\nNow you can use the subgroup CIs (or individual CIs if you have the resources) as stimuli in the CI validation task of Phase II.\n\nThe resulting data is richer and multilevel in nature. You can for instance compute a grand mean for the Trustworthy rating of several Trustworthy subgroup CIs, or compute how the mean rating of each participant deviates from the grand mean, etc. Here’s an example of a potential analysis dataset below.\n\nHow to analyze it will depend on the questions you’re asking. For example, you may be interested in understanding whether the ratings of CIs generated for ‘Trustworthy’ are significantly different from CIs generated for ‘Sad’. Whatever the case, you will have a richer set of data to test your hypotheses if you follow the current recommended practices.\nThanks for reading!\n\n\n\n\nBrinkman, L., Todorov, A., & Dotsch, R. (2017). Visualising mental representations: A primer on noise-based reverse correlation in social psychology. European Review of Social Psychology, 28(1), 333–361. https://doi.org/10.1080/10463283.2017.1381469\nCone, J., Brown-Iannuzzi, J. L., Lei, R., & Dotsch, R. (2021). Type I Error Is Inflated in the Two-Phase Reverse Correlation Procedure. Social Psychological and Personality Science, 12(5), 760–768. https://doi.org/10.1177/1948550620938616\nDotsch R. (2016). Rcicr: Reverse-correlation image-classification toolbox. R package (Version 0.3), 4. https://cran.r-project.org/web/packages/rcicr/index.html\nDotsch, R., & Todorov, A. (2012). Reverse Correlating Social Face Perception. Social Psychological and Personality Science, 3(5), 562–571. https://doi.org/10.1177/1948550611430272\nSchmitz, M., Rougier, M., & Yzerbyt, V. (2021, March 24). Introducing the Brief Reverse Correlation. https://doi.org/10.31234/osf.io/xg693"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Manuel Oliveira",
    "section": "",
    "text": "Here you can find information about my academic research and posts about coding or tools I created for research projects or just for fun.\n\n\n\nWilhelmina Park, Utrecht, Netherlands"
  },
  {
    "objectID": "apps.html",
    "href": "apps.html",
    "title": "Tools",
    "section": "",
    "text": "This shiny app facilitates the generation of the so-called ‘subgroup’ classification images (CIs) for a two-phase reverse correlation methodology.\nRead the post at Blog or Medium\nThe use of subgroup CIs is a currently recommended practice in psychological research involving a two-phase variant of the psychophysical reverse correlation method (e.g. Dotsch & Todorov, 2012). In practice, this implies using the data collected during the first phase (reverse correlation task) to generate multiple group-level CIs associated with the same target construct condition (i.e. several ‘Trustworthy’ subgroup CIs vs. single ‘Trustworthy’ group CI ). These are then rated in the second phase by another group of raters.\nUsing subgroup CIs helps decrease the number of images to rate in the second phase, compared to the alternative but more time consuming option of rating all the individual CIs generated by each participant in the first phase. This approach circumvents the issues (e.g. type I error inflation) associated with using a single group CI in the second phase (see Cone et al., 2021).\nFor more details see: https://github.com/olivethree/shinyrc_subgroupcis"
  },
  {
    "objectID": "publications.html",
    "href": "publications.html",
    "title": "Publications",
    "section": "",
    "text": "2023\nLiefooghe, B., Oliveira, M. J. B., Leisten, L. M., Hoogers, E., Aarts, H., & Hortensius, R. (2023). Are natural faces merely labelled as artificial trusted less? Collabra: Psychology, 9(1): 73066. doi: https://doi.org/10.1525/collabra.73066\n\n\n2022\n[Preprint] Liefooghe, B., Oliveira, M. J. B., Leisten, L. M., Hoogers, E., Aarts, H., & Hortensius, R. (2022, June 10). Faces Merely Labelled as Artificial are Trusted Less. https://doi.org/10.31234/osf.io/te2ju\nOliveira, M., & Garcia-Marques, T. (2022). The effect of facial occlusion on facial impressions of trustworthiness and dominance. Memory & Cognition, Advance Online Publication. https://doi.org/10.3758/s13421-022-01316-z\nGarcia-Marques, T., Oliveira, M., & Nunes, L. (2022). That person is now with or without a mask: how encoding context modulates identity recognition. Cognitive Research: Principles and Implications, 7(29), 1-17. https://doi.org/10.1186/s41235-022-00379-5\n\n\n2020\nOliveira, M., Garcia-Marques, T., Garcia-Marques, L. , & Dotsch, R. (2020). Good to bad or bad to bad? What is the relationship between valence and the trait content of the Big Two? European Journal of Social Psychology, 50(2), 463-483. https://doi.org/10.1002/ejsp.2618\n\n\n2019\nOliveira, M., Garcia-Marques, T., & Dotsch, R. (2019). Combining traits into a face: A reverse correlation approach. Social Cognition, 37(5), 516-545. doi: 10.1521/soco.2019.37.5.516\nOliveira, M., Garcia-Marques, T., Dotsch, R., & Garcia-Marques, L. (2019). Dominance and competence face to face: Dissociations obtained with a reverse correlation approach. European Journal of Social Psychology, 49(5), 888-902. https://doi.org/10.1002/ejsp.2569\n\n\n2017\nSantos, A., Almeida, F., Palma, T., Oliveira, M., & Garcia-Marques, L. (2017). The cultural stereotype of professional groups: Consensus, accessibility and typicality of stereotypic contents. Análise Psicológica, 35, 557-568. https://doi.org/10.14417/ap.1385\n\n\n2016\nRamos, T., Oliveira, M., Santos, A. S., Garcia-Marques, & L. Carneiro, P. (2016). Evaluating young and old faces on social dimensions: Trustworthiness and dominance. Psicológica, 37(2), 169-185.\n\n\n2012\nOliveira, M., & Miranda, M. (2012). Paradigma: Teste de associação implícita [Paradigm: Implicit association test]. Laboratório de Psicologia, 10, 235-249. doi: 10.14417/lp.673\n\n\nPh.D. Dissertation\nOliveira, M.J.B. (2020). The structuring role of valence in the relationship between and within models of face and trait impressions."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Manuel Oliveira",
    "section": "",
    "text": "My name is Manuel Oliveira (he/him). Currently, I’m a postdoctoral researcher at Eindhoven University of Technology working on the topic of AI in Education.\nI have a background in Psychology and my doctoral work focused on the relation between personality stereotypes and social attributions based on facial appearance. Most of the work I’ve done in the past years involved statistics, programming (R & Python), experimental design, and scientific writing. After my Ph.D. I moved on to an R&D industry job where I gained further experience with coding skills, how businesses operate, Artificial Intelligence (AI) applications, and working with teams with diverse professional backgrounds. During that period, I became increasingly interested in the challenges emerging from the growing presence of AI in society. This led me to seek new roles where I could learn more about how people interact with technology in general, and more specifically AI. I found my first opportunity to work on this topic in a postdoctoral position at Utrecht University, where I spent my time research topics of trust in AI and human-AI interaction in general.\nSince 2024, I’m working on the topic of AI in Education as a postdoctoral researcher at Eindhoven University of Technology."
  },
  {
    "objectID": "about.html#experience",
    "href": "about.html#experience",
    "title": "Manuel Oliveira",
    "section": "Experience",
    "text": "Experience\n\nPostdoctoral researcher, Coordination Human-AI Alliance program @ Utrecht University | 2022 - 2023\n\nTeaching activities:\n\nExperimental Methods and Statistics, BSc in Artificial intelligence\nThesis supervision, BSc in Artificial intelligence\nMethods for AI Research, MSc in Artificial Intelligence\nIntegrative Practicum - Scientific Writing, Correlational Designs and Data Analysis, Research MSc Psychology\n\n\nData Modeller & Researcher @ Philips - Digital Cognitive Diagnostics | 2020 - 2022\nTeaching Assistant @ ISPA - Instituto Universitário | 2016 - 2019\n\nAcademic Skills\nData Processing\nExperimental design\nFace image manipulation\n\nResearch Assistant @ ISCTE-IUL | 2014\nTeaching Assistant @ University of Lisbon, Faculty of Psychology | 2013 - 2014\n\nFace image manipulation techniques\n\nResearch Assistant @ University of Lisbon, Faculty of Psychology | 2012 - 2014\nStrategic Planning Internship @ Publicis Groupe | 2009 - 2010"
  },
  {
    "objectID": "about.html#education",
    "href": "about.html#education",
    "title": "Manuel Oliveira",
    "section": "Education",
    "text": "Education\nPhD in Social Psychology | 2015 - 2019\n\nISPA - Instituto Universitário | Lisbon, Portugal & Utrecht, Netherlands\n\nGraduated with Honor and Distinction\nPhD Dissertation: “The structuring role of valence in the relationship between and within models of face and trait impressions”\n\n\nMSc in Social and Organizational Psychology | 2009 - 2011\n\nISPA - Instituto Universitário | Lisbon, Portugal\n\nBSc in Psychology | 2006 - 2009\n\nUniversity of Aveiro | Aveiro, Portugal"
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Title\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\nDate\n\n\nTitle\n\n\nAuthor\n\n\nReading Time\n\n\n\n\n\n\n\n\n\nJune 11, 2023\n\n\nUsing Reverse Correlation Methods in Psychology Research: R tutorial and Recommended Practices\n\n\nM. Oliveira\n\n\n18 min\n\n\n\n\n\n\n\nAugust 22, 2022\n\n\nHi, my name is…\n\n\nM. Oliveira\n\n\n2 min\n\n\n\n\n\n\nNo matching items"
  }
]