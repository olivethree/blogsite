[
  {
    "objectID": "posts/p_1/index.html",
    "href": "posts/p_1/index.html",
    "title": "Hi, my name isâ€¦",
    "section": "",
    "text": "I love coffee and would ideally invite you to join me for a cup or two. To compensate, hereâ€™s a picture of a morning coffee scene to get you in the mood to read the rest of the monologue:\n\n\n\nRetrieved from: pexels.com\n\n\nIn 2019, I earned my PhD in Social Psychology from ISPA - Instituto Universitario. During my PhD I divided my time between Portugal and the Netherlands, conducting research at William James Center for Research and Utrecht University.\n\n\n\nMoments after my PhD defense ðŸ˜„ - 16 December 2019\n\n\nMy doctoral work resulted in contributions to the fields of social perception and psychophysical image classification methods like reverse correlation.\nAfter obtaining my PhD, I moved into an industry job in pursuit of research with a more direct societal impact. I like to believe that goal was achieved during the time I spent at a Philips venture working on the development of automated clinical decision support systems. During this time my interest in the topic of human technology interaction awakened and I took an interest in roles where I could deepen my knowledge about the topic, and perhaps contribute to the domain in some way.\nI found my first opportunity to pursue the new interest in a postdoc position at Utrecht University focusing on aspects of trust in AI and human-AI interaction in general. During this period I faced some challenges (naturally): adjusting back to the academic environment, catching up with a rapidly growing field of interdisciplinary research outside of the comfort zone (social cognition), and learning how to effectively split my time between research and teaching. At the same time, I learned much about how academics approach the topic of human-AI interaction, learned much about the relevance of Psychology knowledge to the impact of AI in society, and more importantly, met wonderful people including students and colleagues with whom I learned, and keep learning, from.\nThe emergence of ChatGPT by the end of 2022 had a strong impact on the research I was conducting, raising new questions and opening up new opportunities to do research. But arguably more important was the impact of generative AI chatbots on teaching practices, which prompted us all (or should have!) to start discussing the future of Education in a world increasingly infused with tools capable of mimicking human output. The impact of AI in Education sounded like a meaningful question to invest my time in. Currently, I am doing a second postdoc at Eindhoven University of Technology. There, I spend most of my time thinking about the impact of AI in Education together with an interdisciplinary team of researchers (Economics, Philosophy, Education), university staff and students.\nIn my free time, I enjoy playing music, hunting for specialty coffee, modern and retro games, catch up with my to-read list and travelling."
  },
  {
    "objectID": "posts/p_2/index.html",
    "href": "posts/p_2/index.html",
    "title": "Using Reverse Correlation Methods in Psychology Research: R tutorial and Recommended Practices",
    "section": "",
    "text": "If you found this post, chances are you might already be familiarized with the reversed correlation (RC) methodology in psychological research. But in case you never heard about it, are just curious, or need to refresh your memory, I provide a brief overview of the method below, along with R code you can use to implement the method from the start.\nThe main motivation behind this post is to offer an up-to-date tutorial on how to implement the two-phase variant of the RC methodology following the latest recommended practices. While doing so, I will show you the steps you can take to avoid a well-known limitation of this method.\n\n\nBelow I describe the steps typically involved in a two-phase reverse correlation methodology in the domain of social psychological research (see for example, Dotsch & Todorov, 2012).\nThis methodology has been often used to examine research question social judgments based on facial appearance, such as â€˜Which facial cues predict that a person will be judged as having Portuguese nationality?â€™ or â€˜Which facial cues predict that a person will be judged as trustworthy?â€™. Naturally, you could ask the same question about things other than faces, such as what features of an object predict that we judge it as a â€˜chairâ€™. However, for the sake of simplicity, the current overview will focus on the example of social judgments from facial appearance.\nResearch question: Imagine youâ€™re interested in visualizing the facial cues that people use to judge someone as a trustworthy person.\n\n\n\nThe image below illustrates the 3 main steps of Phase I.\n\n\n\nThe first thing you need to implement the method is to find an image of a face. This is called the base image. There are many variables you may want to take into consideration during the selection process (for details see Brinkman et al., 2017), but picking an image of a face is what youâ€™d go for here. This can be seen as a trick to approximate the image to what we think is the â€˜prototypicalâ€™ image of the thing we want to study, which in our example is a face conveying trustworthiness. For this post, I used a randomly generated deep fake image of a female face (using thispersondoesnotexist.com). You can copy-paste it from below:\n\nTo implement the method you can use the rcicr R package (Dotsch, 2016), along with a couple of other useful R packages for data and image processing. In the R tutorial below I show you how you can setup an R project, prepare the base image to use with rcicr, generate task stimuli, and generate classification images (CIs) for different analytical scenarios (older approach vs.Â latest recommendations).\nRequirements: This tutorial assumes you have R and RStudio installed (R version used in this tutorial is 4.2.0, but I expect later versions to work just as well).\n\n\nI highly recommend you create a folder in your computer called â€œRworkspaceâ€ or any other name you prefer, where you can store all your R projects inside. This makes it easier to organize and find your multiple projects and contents.\nNext, open RStudio, go to File > New Project\n\nNow you can select New Directory > New Project\nIn the window below, use Browse to find your â€œRworkspaceâ€ folder. Once you set it, you can now enter the name of your R project under Directory name. I named my project â€œrc_tutorialâ€. This will create a folder named after the project name inside the parent folder Rworkspace. Inside your R project folder you will find a .Rproj file. This .Rproj file teels R that the root directory of your project is where it is located.\nFor now, you can simply proceed without ticking any of the options as shown below (unless you are familiarised with git version control or want to have better control of your R environment).\nClick Create Project and youâ€™re ready to start coding!\n\nTo start coding open a R script (File > New File) and save it.\nTo install packages you can use the following function in the RStudio console:\n\ninstall.packages(\"PACKAGE_NAME_HERE\")\n\nIf you have a recent version of the RStudio it will automatically detect the packages that are required by a script and will prompt you to install them. You should start by loading all the required packages at the beginning of a script. Youâ€™ll need these:\n\n# Required packages for this tutorial\nlibrary(here) # for easy path referencing\nlibrary(tidyverse) # my preference for data processing, improved code readability\nlibrary(magrittr) # to pipe code\nlibrary(devtools) # required to load github based libraries\nlibrary(imager) # to process and edit images\n\n# Automatically detect if rcicr package (v1.0.0 -- github version) is installed. \n# If yes, loads it. Otherwise installs it and loads it.  \nif(!require(rcicr) | packageVersion(\"rcicr\") != \"1.0.0\") {\n  devtools::install_github('rdotsch/rcicr') \n  library(rcicr)\n}\n\nThe rcicr package requires you to convert the base image to grayscale, and to resize it to identical width and height (e.g.Â 512 by 512 pixels).\n\n# Path to image file ------------------------------------------------------\n\n# if image is in R project root directory (where Rproj file is located)\n# simply enter the filename as a string\nimg_path <- \"generated-image-bxx2g8.jpg\" \n\n# otherwise you can use the here() package to specify the path to the file\n# starting from the R project root directory\n# in the commented example below, the file would be stored in a folder named 'subfolder_X' located in the root directory\n\n# img_path <- here::here(\"subfolder_X\", \"generated-image-bxx2g8.jpg\") \n\n# Image dimensions --------------------------------------------------------\n\n# Set width in pixels\nimg_width <- 512\n\n# Automatically set height to match width (required for rcicr) \nimg_height <- img_width\n\n\n# Convert to grayscale and resize image -----------------------------------\n\nbase_image <- load.image(\"generated-image-bxx2g8.jpg\") %>% \n  imager::grayscale() %>% \n  imager::resize(size_x = img_width, size_y = img_height)\n\n\n# Save image --------------------------------------------------------------\n\nsave.image(im = base_image, \"gray_base_image.jpg\")\n\n\n\n\n\nNow you can use the base image to generate many pairs of images. These pairs of images are the stimuli you need for the 2-image forced choice task. In each pair, one image is the base image with superimposed visual noise, and the other image is the base face with the inverted noise from the first image (its negative image). The use of noise + inverted noise is a trick to maximize the differences between the images in any given task trial. This minimizes the difficulty of having to choose between highly similar images. For this example, Iâ€™m generating 500 image pairs, for a task with 500 decision trials. Hereâ€™s how you can generate them:\n\n# Load base image file ----------------------------------------------------\n\nbase_face_file <- list(\"base_face\" = \"gray_base_image.jpg\")\n\n\n# Generate image pairs ----------------------------------------------------\n\n# How many trials do you wish to have in your task?\nnumber_task_trials <- 500\n\n# Which type of visual noise do you want to generate?\n# Default is sinusoid; other options include Gabor noise  \nselected_noise <- \"sinusoid\" # or \"gabor\"\n\n# Generate stimuli\ngenerateStimuli2IFC(base_face_file, \n                    n_trials = number_task_trials,\n                    img_size = 512,\n                    noise_type = selected_noise,\n                    stimulus_path = \"./stimuli\")\n\n# WHERE ARE STIMULI STORED?\n\n## The generated stimuli can be found in a folder named 'stimuli' (or name you\n## used in stimulus_path argument above)\n\n## In this folder you can also find the .Rdata file file needed for analysis of\n## data after data collection. This .Rdata file contains the parameters that \n## were used to generate each stimulus\n\n\n\n\nNow that you have your stimuli, you can use them in a 2-image forced choice task â€” as we have only two images per trial, but there are other variants of the task with more images per trial (see Schmitz et al., 2021). Please note that each image pair is composed of an original image and its inverted version (its visual negative).\nUse your preferred experiment-building program to run the task. Some popular choices include Inquisit, PsychoPy, OpenSesame, Gorilla, or Qualtrics. In the image below you can see an example of a 2-image forced choice task. I highlight the essential components and the expected content of the data output, but please add Informed Consent in the beginning, and demographics (+ debriefing if necessary) at the end.\n\nImportant to take into account while building the task:\n\nImage pair presentation. Each image pair generated in Step 2 includes an image labeled as original (e.g.Â rcic_base_face_1_00001_ori.png) and another labeled as inverted (e.g.Â rcic_base_face_1_00001_inv.png). These should be presented as a pair in the same task trial. Therefore, each trial number will be associated with a specific pair of stimulus images.\nCounterbalance ori and inv images. You may want to counterbalance the position where the ori and inv images are presented, between-participants. That is, for half the participants the ori images are displayed at the left (inv at the right), and for the other half the ori images are displayed at the right (inv at the left).\nRequired data output. Your program needs to log the following information: condition of the target construct youâ€™re interested in (e.g.Â trustworthy), unique subject ID, trial number, image presented on the left, image presented on the right, image selected, whether image is ori or inv, response (derived from the type of image selected (ori = +1, inv = -1). See the example data output above. The most important data to have in the end are: subject, trial number, and response coded as 1 or -1.\n\nFor the sake of this tutorial, I generated some bogus response data to use in the next steps. You can also generate it using the code below.\n\n# Simulate RC response data -----------------------------------------------\n\n# How many participants?\nnr_subjects <- 50\n# How many trials?\nnr_trials <- 500\n\n# Generate simulated data frame\nsim_data <- data.frame(\n  condition = \"TRUSTWORTHY\",\n  subject = rep(1:nr_subjects, each = nr_trials),\n  trial = rep(1:nr_trials, times = nr_subjects),\n  resp = rbinom(nr_subjects*nr_trials, 1, 0.3)\n) %>% \n  mutate(resp = ifelse(resp == 0, -1, 1))\n\n# Save data\nwrite_csv(sim_data, file = \"sim_rc_data.csv\")\n\n\n\nOnce you have finished collecting and processing your data, you are ready to generate classification images (CIs). You have several options:\n\nCreate a CI for each participant: we call this an Individual CI\nCreate a CI using the data from all participants of the same target construct condition (e.g.Â TRUSTWORTHY): we call this a Group CI\nCreate several CIs using randomly sampled subsets of participants from the same target construct condition (e.g.Â TRUSTWORTHY) we call this a Subgroup CI\n\nUntil recently, most studies using the two-phase variant of RC methods in Social Psychology, focused on generating and analyzing group CIs, as these capture the visual features predicting the target construct (TRUSTWORTHY) on which all participants of a condition agree on average.\n\nFor now, letâ€™s focus on the traditional options of CI generation and how to do it in R.\n\n\n\n\n# Setup -------------------------------------------------------------------\n\n# Load response data\nrc_data <- vroom::vroom(\"sim_rc_data.csv\") # replace with your data filename if you're not using the tutorial bogus data\n\n# Convert data to data.frame format to prevent issues with rcicr package\nrc_data <- as.data.frame(rc_data)\n\n# Load Rdata file with stimulus generation info\n# File is stored inside the task stimuli folder when stimuli are generated\nrdata_file_location <- here::here(\"stimuli\", \"rcic_seed_1_time_Jun_10_2023_18_47.Rdata\") \n\n# Label given for base face at stimulus generation, in our case it was 'base_face'\nbase_face_label <- \"base_face\"\n\n\n# Generate Individual CIs for all participants ---------------------------------------------\n\nind_cis <- batchGenerateCI(\n  data = rc_data,\n  by = \"subject\",\n  stimuli = \"trial\",\n  responses = \"resp\",\n  baseimage = base_face_label,\n  label = \"ind_ci_\",\n  rdata = rdata_file_location,\n  targetpath = \"./individual_cis\"\n)\n\n# You will find all the individual CI image inside the folder 'individual_cis'\n\n\n# Generate Individual CI for a single participant ------------------------------------------------------\n\n# Here I'm subsetting the data from participant nr. 25\ns25_df <- rc_data %>% filter(subject == 25)\n\n# To customize the filenames of output individual CI\nparticipant_nr <- 25\n\n# Generate individual CI\nindci_25 <- generateCI(\n  stimuli = s25_df$trial,\n  responses = s25_df$resp,\n  baseimage = base_face_label,\n  filename = paste0(\"indci_\", participant_nr),\n  rdata = rdata_file_location,\n  targetpath = \"./individual_cis\"\n)\n\n# You will find the individual CI image inside the folder 'individual_cis'\n\n\n\n\nFor the tutorial example condition â€˜TRUSTWORTHYâ€™:\n\n# Setup -------------------------------------------------------------------\n\n# Load response data\nrc_data <- vroom::vroom(\"sim_rc_data.csv\") # replace with your data filename if you're not using the tutorial bogus data\n\n# Convert data to data.frame format to prevent issues with rcicr package\nrc_data <- as.data.frame(rc_data)\n\n# Load Rdata file with stimulus generation info\n# File is stored inside the task stimuli folder when stimuli are generated\nrdata_file_location <- here::here(\"stimuli\", \"rcic_seed_1_time_Jun_10_2023_18_47.Rdata\") \n\n# Load rdata into environment\nbase::load(rdata_file_location)\n\n# Label given for base face at stimulus generation, in our case it was 'base_face'\nbase_face_label <- \"base_face\"\n\n\n# Generate group CI for Trustworthy --------------------------------------------------\n\ngroup_cis <- batchGenerateCI(\n  data = rc_data,\n  by = \"condition\",\n  stimuli = \"trial\",\n  responses = \"resp\",\n  baseimage = base_face_label,\n  rdata = rdata_file_location,\n  targetpath = \"./group_cis\"\n)\n\n# You will find the individual CI image inside the folder 'group_cis'\n\nTraditionally, once you have the group CI for your different target conditions (e.g.Â trustworthy, untrustworthy, happy, sad) you can move to Phase II.\n\n\n\n\n\nIn this phase, you are usually interested in understanding if the CIs you generated in Phase I contain the signal or pattern of features that are predictive of the judgments you are studying (i.e., trustworthy, etc.). Although you can visually inspect the CIs and reach your own conclusions about whether they communicate the signal you are interested in, this would not be very reliable, as you are more biased than you think (confirmation biasâ€¦look it up!). However, you could ask an entirely new group of people to make judgments about your fresh CIs. Importantly, and ideally, these people should not have participated in the RC task in Phase I.\nTraditionally, researchers focused on collecting ratings for the group CIs of their conditions. If a group CI was generated for the concept of a â€˜trustworthyâ€™ face, then the new group of participants would evaluate how trustworthy this group CI face looks to them. If the new group of participants indeed rates this group CI as trustworthy, researchers can pop a bottle of champagne and report to the world that the group CI captured the visual cues associated with judging a face as â€˜trustworthyâ€™.\n\nAll good right?\nWellâ€¦\n\n\nA group of psychology researchers (Cone et al., 2021) called attention to the fact that evaluating a single group CI in the second phase is not ideal, as a single group CI is not able to convey any of the variability of peopleâ€™s results in the first phase. This is important to consider since different groups of participants will generate slightly different group CIs for the same construct, and therefore, presenting the group of raters in the second phase with a single group CI is like presenting a randomly drawn card from the deck without showing them how the cards can vary.\nIn phase one, there is an individual result â€” or individual CI â€” for each participant. The group CI created by aggregating all the individual CIs from a given condition (for instance, â€˜Trustworthyâ€™ face) averages out any of the individual differences present in the individual CIs (how each participant imagines a â€œtrustworthyâ€ face to look like), keeping only the visual information that is common across the individual CIs.\nWhile there is nothing fundamentally wrong with asking a second group of participants to judge a single group CI on â€˜trustworthinessâ€™, there is an unfortunate loss of information regarding how this single image could vary depending on the visual cues that different individuals or subgroups of them (they are all sampled from a larger population after all) could have used to make their decisions in the RC task. In this way, the ratings in Phase II can capture what was common across all the individual CIs, but cannot pick up on any of the variations that occurred in peoplesâ€™ responses during Phase I.\n\n\n\nTaken to the extreme, the solution could also involve asking the CI raters in phase two to rate all of the individual CIs, thus preventing any loss of variability. However, when you have dozens if not hundreds of individual CIs, your study becomes very expensive as participants are usually paid according to the duration of the study. Plus, the task becomes increasingly tiresome for the participants who might then fall into the temptation of randomly responding.\n\nAnother recently proposed solution involves reducing the number of CIs associated with the same condition while keeping a decent amount of them. We call these subgroup CIs. This solution represents a compromise between the costly option of using all individual CIs and the problematic option of using a single group CI. The creation of subgroup CIs involves taking random subsets of participants from the same condition and generating a (sub)group CI for each of these subsets. This increases the scientific soundness and reliability of the results of Phase II, while saving us time and financial resources.\n\nIn sum, it is currently recommended that you use either all individual CIs, or subgroup CIs in Phase II.\n\nFor more detailed and formal explanations see Cone et al.Â (2021).\n\n\n\nThere are a couple of decisions you have to make before you start generating subgroup CIs for Phase II, including:\n\nHow many subgroup CIs you want? The maximum you can have in practice = number of all participants in condition - 1. You will want less than that I suppose (otherwise, whatâ€™s the point?).\nWhat percentage of participant IDs do you want to randomly draw from the full set for each subgroup CI? I recommend using either: 0.25, 0.50, or 0.75. Note that 1.00 (100%) is equivalent to requesting a group CI, as all participants from the condition will be used.\n\n\n# Setup -------------------------------------------------------------------\n\n# Load response data\nrc_data <- vroom::vroom(\"sim_rc_data.csv\") # replace with your data filename if you're not using the tutorial bogus data\n\n# Convert data to data.frame format to prevent issues with rcicr package\nrc_data <- as.data.frame(rc_data)\n\n# Load Rdata file with stimulus generation info\n# File is stored inside the task stimuli folder when stimuli are generated\nrdata_file_location <- here::here(\"stimuli\", \"rcic_seed_1_time_Jun_10_2023_18_47.Rdata\") \n\n# Load rdata into environment\nbase::load(rdata_file_location)\n\n# Label given for base face at stimulus generation, in our case it was 'base_face'\nbase_face_label <- \"base_face\"\n\n\n# Subgroup CI generation --------------------------------------------------\n\n# Set seed for reproducible results (default = 42, geeks will know the 'meaning' of this)\ncustom_seed <- 42\nset.seed(custom_seed)\n\n# Derive total number of participants\nparticipant_n <- rc_data$subject %>% unique %>% length\n\n# How many possible individual CIs\ntotal_ind_cis <- participant_n # This step is explicitly added for the sake of clarity\n\n# Desired number of sub group-CIs (using 4 for this tutorial)\nnr_groupcis <- 4\n\n# Fraction of participants (individual CIs) to randmoly sample from the full set\n# for each subgroup CI\n# I recommend using 0.25, 0.50, or 0.75. \n# Important: using 1.00 would be equivalent to a group CI.\ninput_fraction <- 0.50\n\n# Initialize dataframe to log information about sampled individual CIs\nsampled_cis <- NULL\n\n# Loop over \nfor(i in 1:nr_groupcis) {\n  \n  # Sampling (without replacement)\n  sampled_ind_cis <- rc_data$subject %>%\n    unique() %>%\n    sample(size = round(input_fraction * participant_n, 0),\n           replace = FALSE)\n  \n  # Filter input response data set to include only subset of participants (individual CIs)\n  subgroup_df <-\n    rc_data %>% filter(subject %in% sampled_ind_cis) %>% mutate(subgroup_ci_nr = i)\n  \n  # Subgroup CI name\n  subgroupci_label <- paste0(\"subgroup_ci_\", i)\n  \n  # Log sampled data\n  sampled_cis <- rbind(sampled_cis, data.frame(subgroupci_label, sampled_ind_cis))\n  \n  \n  # Generate subgroup CI\n  sg_ci <- batchGenerateCI(\n    subgroup_df,\n    by = \"subgroup_ci_nr\",\n    stimuli = \"trial\",\n    responses = \"resp\",\n    baseimage = base_face_label,\n    rdata = rdata_file_location,\n    save_as_png = TRUE,\n    targetpath = \"./subgroup_cis\",\n    antiCI = FALSE,\n    scaling = \"autoscale\",\n    constant = 0.1,\n    label = subgroupci_label\n  )\n}\n\n# You can find the subgroup CIs inside the folder 'subgroup_cis'\n\n# Save data file with onformation about which participant Ids were sampled for each subgroupCI\nwrite_csv(sampled_cis, file = \"pids_subgroupcis_data.csv\")\n\n\n\nAlternatively, when you already have the Phase I task response data and want to quickly generate a dataset that you can use to generate subgroup CIs, you can bypass some of the initial coding and use the shiny app below. However, the main benefit of using this app is to speed things up, and you still need to go back to R coding after getting the output from it.\nðŸ¤” Currently wondering if this app is really that useful, since most of you will be good at coding if you got into RC methodsâ€¦\nShiny app: https://github.com/olivethree/shinyrc_subgroupcis\n\n\n\n\n\nNow you can use the subgroup CIs (or individual CIs if you have the resources) as stimuli in the CI validation task of Phase II.\n\nThe resulting data is richer and multilevel in nature. You can for instance compute a grand mean for the Trustworthy rating of several Trustworthy subgroup CIs, or compute how the mean rating of each participant deviates from the grand mean, etc. Hereâ€™s an example of a potential analysis dataset below.\n\nHow to analyze it will depend on the questions youâ€™re asking. For example, you may be interested in understanding whether the ratings of CIs generated for â€˜Trustworthyâ€™ are significantly different from CIs generated for â€˜Sadâ€™. Whatever the case, you will have a richer set of data to test your hypotheses if you follow the current recommended practices.\nThanks for reading!\n\n\n\n\nBrinkman, L., Todorov, A., & Dotsch, R. (2017). Visualising mental representations: A primer on noise-based reverse correlation in social psychology. European Review of Social Psychology, 28(1), 333â€“361. https://doi.org/10.1080/10463283.2017.1381469\nCone, J., Brown-Iannuzzi, J. L., Lei, R., & Dotsch, R. (2021). Type I Error Is Inflated in the Two-Phase Reverse Correlation Procedure. Social Psychological and Personality Science, 12(5), 760â€“768. https://doi.org/10.1177/1948550620938616\nDotsch R. (2016). Rcicr: Reverse-correlation image-classification toolbox. R package (Version 0.3), 4. https://cran.r-project.org/web/packages/rcicr/index.html\nDotsch, R., & Todorov, A. (2012). Reverse Correlating Social Face Perception. Social Psychological and Personality Science, 3(5), 562â€“571. https://doi.org/10.1177/1948550611430272\nSchmitz, M., Rougier, M., & Yzerbyt, V. (2021, March 24). Introducing the Brief Reverse Correlation. https://doi.org/10.31234/osf.io/xg693"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Manuel Oliveira",
    "section": "",
    "text": "Here you can find information about my academic research and posts about coding or tools I created for research projects or just for fun.\n\n\n\nWilhelmina Park, Utrecht, Netherlands"
  },
  {
    "objectID": "apps.html",
    "href": "apps.html",
    "title": "Reverse Correlation: Sampling Subgroup CIs",
    "section": "",
    "text": "Ethical guidance for GenAI-assisted academic writing\nLaunch the CARefully Prompt Library\nThis interactive prompt library was developed to support PhD students and early-career researchers in navigating the complex ethics of using Generative AI in academic writing activities. This tool was developed for a workshop at the 2025 SEFI Summer School(20-23 May @ Den Bosch, The Netherlands), it operationalizes the recent ethical framework by Cheng et al.Â (2025) to help writers make informed decisions.\n\nEthical tiers: Prompts are classified (Tier 1-3) based on Cheng et al.â€™s taxonomy, helping you distinguish between â€œsafeâ€ tasks (e.g., editing) and those requiring heavy oversight (e.g., ideation).\nOffers a structured starting point: While not an empirically validated tool (feel free to use it in your educational pilot studies though!), this library offers a structured â€œsandboxâ€ of templates and examples. Its goal is to inspire better prompting habits and provide a concrete way to reflect on the ethical weight of different writing tasks.\nWriting goal-oriented : Browse prompts for specific needs like for example Improving Clarity, Drafting Assistance, or Maintaining Authorial Voice."
  },
  {
    "objectID": "apps.html#brief-reverse-correlation-task",
    "href": "apps.html#brief-reverse-correlation-task",
    "title": "Reverse Correlation: Sampling Subgroup CIs",
    "section": "Brief Reverse Correlation Task",
    "text": "Brief Reverse Correlation Task\nRun the Brief Reverse Correlation Task locally in your browser (no internet required).\nView Project on GitHub | View Web Demo\nIn this demo you can see how the full task works and obtain actual results you can use for learning. Using the target category â€œStar Wars Fanâ€ for the sake of demonstration. Prefer Star Trek? Let me know and Iâ€™ll make that version! :)\n\n\n\n\n\n\nFull Stimuli Generation Guide & R Script\n\n\n\n\n\nTutorial: Generating stimuli for the task\nSelect a base face image. In this example, I use the average of the average male and average female faces, facing forward and with neutral expression, from the Karolinska Face Database (Lundqvist, Flykt, & Ã–hman, 1998).\nNext, you need to decide how many trials you need for your task. In a traditional 2-image forced-choice Reverse Correlation task, we need a pair of images per trial. Therefore, the total images you need to generate as stimuli for the task = [number of trials you want] x [number of images per trial]. For the case of 2 images per trial, this would be 600 images for 300 trials.\nHowever, the benefit of using Brief RC is so you can present more images per trial, thereby reducing the traditional massive number of trials of 2 image forced-choice RC tasks (see Schmitz et al., 2024 for more details). In this tutorial I focus on the version with 12 images per trial. While there is no specific rule to define how many images you can use in the Brief RC task, 12 images per trial is, in my opinion, a reasonable amount to present in most devicesâ€™ screens, while keeping a manageable informational load for the participants.\nIn this version of the task, which I refer to as Brief RC 12, 12 images are presented in each trial. These are not just any images randomly drawn from the pool of stimuli. Instead, these are actually 6 pairs of images, where each pair is associated with a specific number (assigned during the noise generation procedure) and includes an original noise patch (labelled as ori) and its inverted noise (labelled as inv).\nIn this case, I will follow the trials used by the Brief RCâ€™s authors (Schmitz et al., 2024) for the Brief RC 12 task:\nTotal trials: 60 Images per trial: 12 (6 pairs of ori-inv images) Total images to generate = 720\nFinally, the generated images are resized for proper fit within the screen area. Image stimuli in the RC task (used in psychology research) are frequently 512x512 px, or 256x256, or 128x128. Here, we will use 128x128 as the final output size for the stimulus images. Importantly, your base face image will need to be resized to these dimensions as well, and make sure this does not stretch or alter the original base face image in any way (unless you have any reasons to distort the natural configuration of a face).\nR script to generate stimuli Note: R scripts are adapted from the original materials shared by Schmitz et al.Â (2024).\n\n# Required packages\nlibrary(tidyverse)\nlibrary(data.table)\nlibrary(magick)\nlibrary(magrittr)\n\n# Github packages\nlibrary(devtools)\n# rcicr\nif(!require(rcicr)) devtools::install_github('rdotsch/rcicr'); library(rcicr)\n\n# Parameters\n{# Seed for reproducibility}\ngen_seed   &lt;- 1984\n\n# Define how many task trials you want\nnr_task_trials &lt;- 60\n\n# Number of image pairs (ori + inv)\nnr_rc_trials &lt;- nr_task_trials * 6\n\ncat(\"Setting script to generate stimuli for a total of: \", nr_task_trials, \"trials (Brief RC 12 version).\\n\")\ncat(\"This means you will have a total of \", nr_rc_trials, \"image pairs, or \", nr_rc_trials*2, \"total amount of images.\")\n\n# Path for base face image file (average of the average male and average female face of the Karolinska face database, neutral expression, frontal pose)\n\npath_baseface &lt;- \"FMNES.jpg\" # Replace with your own base image\n\n# Output directory to store the stimuli images\noutput_path  &lt;- \"img/\"\n\n# Stimuli generation\nnoise_matrix &lt;- rcicr::generateStimuli2IFC(\n  base_face_files     = list('avg' = path_baseface),\n  n_trials            = nr_rc_trials,\n  seed                = gen_seed,\n  save_as_png         = TRUE,\n  stimulus_path       = output_path,\n  return_as_dataframe = TRUE,\n  save_rdata          = FALSE\n)\n\n# Noise matrix\n\n# Convert noise matrix to a data.table object\nnoise_matrix &lt;- data.table(noise_matrix)\n\n# save it as txt file\nfwrite(noise_matrix, file = \"noise_matrix.txt\", sep = \" \", row.names = FALSE, col.names = FALSE)\n\n# Rename image files\n\n# Ori files\nlist.files(path = output_path, pattern = \"ori.png\", full.names = TRUE) %&gt;%\n  set_names(paste0(output_path, \"faceOri\", seq_along(.), \".png\")) %&gt;%\n  walk2(., names(.), file.rename)\n\n# Inv files\nlist.files(path = output_path, pattern = \"inv.png\", full.names = TRUE) %&gt;%\n  set_names(paste0(output_path, \"faceInv\", seq_along(.), \".png\")) %&gt;%\n  walk2(., names(.), file.rename)\n\n# Resize image files for brief RC task\n\n# Define image size (original brief RC paper used 150 x 150 px)\nnew_size &lt;- \"150\"\n\n# Create the 'resized' directory if it doesn't exist\ndir.create(file.path(output_path, \"resized\"), showWarnings = FALSE)\n\n# Process and move images to 'resized' \nlist.files(output_path, pattern = \"face\", full.names = TRUE) %&gt;%\n  walk(function(img_path) {\n    outpath &lt;- gsub(\".png\", \"s.png\", img_path)\n    resized_path &lt;- file.path(output_path, \"resized\", basename(outpath))\n    image_read(img_path) %&gt;%\n      image_scale(new_size) %&gt;%\n      image_write(path = resized_path, format = \"png\")\n  })\n\nOnce the generation is completed, you can find the image stimulus set in â€œimg/resizedâ€. All you need to do next is to copy paste these images to the appropriate experiment folder containing the images (see below).\nImportantly, this script generates the noise matrix file (very large file), that contains the information you need to compute the classification images after you run the experiment.\n\n\n\n\nSetting up the experiment\nThese instructions outline how to set up and run the Brief Reverse Correlation task developed by Schmitz et al.Â (2024) on your local machine.\n\n\n\n\n\n\nSetup Instructions\n\n\n\n\n\n\n1. Create a project folder:\nBegin by creating a new folder on your computer to house the task files. Choose a descriptive name for this folder (e.g., Brief_RC_Task, BRC_Experiment, or similar). This will help keep your files organized.\n\n\n2. Download files from GitHub:\nNavigate to the GitHub repository: https://github.com/olivethree/briefRC. You will need to download the following:\n\nHTML File: Download the HTML file that contains the application code.\nimages folder: Download the entire images folder, including all its contents (the image files used in the task). It is crucial to maintain the folder structure; do not just download the images individually.\n\n\n\n3. Project structure:\nPlace the downloaded HTML file and the images folder (with its contents) directly into the project folder you created in Step 1. The structure should look like this:\nBrief_RC_Task/       (Your project folder)\nâ”œâ”€â”€ index.html      (Example HTML filename, but can be for example demo_briefrc_12.ENG.html)\nâ””â”€â”€ images/         (The images folder, do not change the name of this folder)\n    â”œâ”€â”€ *.png       (filenames follow a strict format like for example faceOri&lt;number&gt;.png)\n    â”œâ”€â”€ *.png\n    â””â”€â”€ ...          (Other image files)\n\n\n4. Adjust experiment content to your needs:\nYou can adjust the informed consent, task instructions, and trial instruction (very important as this is the target category you are interested in, e.g.Â Select the face that looks like &lt;YOUR_CATEGORY_OF_INTEREST&gt;).\nTo adjust these instructions, you can simply edit the content of the HTML file by opening it in your favorite IDE (e.g.Â Visual Code, Notepad++, Xcode, etc.) or text editor, and look for the text. In case you get lost, you can ask ChatGPT or similar chatbots for guidance on where to find this content (or even change it in a more efficient way through prompt engineering if you know what youâ€™re doingâ€¦just remember to be critical of the output of generative AI, always verify!).\n\n\n5. Run the experiment\nOpen the HTML file (e.g., index.html) in your preferred web browser (e.g., Chrome, Firefox, Safari, Edge). The experiment should now load and be ready to use.\n\n\n6. Results:\nUpon completion of a session, the results will be automatically saved to your browserâ€™s default downloads folder (typically named â€œDownloadsâ€).\n\n\n\n\n\n\nGenerating Individual Classification Images from Brief RC task data\nIn this section I provide an example of how to generate an individual classification image based on the data output of the Brief RC task. I will use the output format of the web version of the task I describe above (https://olivethree.github.io/briefrc12online/).\n\n\n\n\n\n\nData Analysis Code & Reference\n\n\n\n\n\nNote: The code below only covers the generation of individual classification images. You can also generate Group CIs using the data from multiple participants, and loading it into R by combining into a single dataset. The data required for Group CI generation includes an additional column (string/factor) specifying target judgment condition, such as Star Wars Fan or Trustworthy. This additional column can then be used to compute the CI with the aggregated condition level data.\nThe output from the web version of the task I indicate above includes the following data:\n\n\n\n\n\n\n\nColumn\nDescription\n\n\n\n\nparticipant_number\n[Essential] Identifier for each participant, provided by the user/experimenter in the first screen.\n\n\nresponse_id\n[Essential] Unique identifier for each response. Can be used together with (or instead of) participant_number, point being you need a unique identifier for each participant.\n\n\nic_agree\nWhether the participant agreed to informed consent (True/False)\n\n\ntrial\n[Essential] The trial number in the experiment. In this case, a total of 60 trials (per participant).\n\n\nselected_image\nFile name of the specific image chosen by the participant during the trial\n\n\nimage_number\n[Essential] A numeric identifier for the selected image. This is crucial information, connected to the noise matrix text file created during the generation of the CIs for the task (see Script 1)\n\n\nimage_type\nIndicates the type of image selected. Therefore, this is also the RESPONSE, reflecting the participantâ€™s decision.\nIn a trial there are several pairs of images. Each pair is associated with a specific number (i.e., image_number). One of the images in the pair is original noise (coded as +1), the other is the inverted noise (coded as -1).\n\n\ntimestamp\nTimestamp of trial start.\n\n\nimage_1 to image_12\nThe 12 file names of the images displayed during the trial. Useful to verify if the 12 images correspond to six pairs of ori-inv images.\n\n\nage\nParticipantâ€™s age.\n\n\ngender\nParticipantâ€™s gender.\n\n\n\nNot that if your data looks different, you need to tweak the code to adapt it to your case (the moidified function should always stay the same though).\n\nModified function to generate BriefRC CIs\nCreate a .R file with this content (derived from Schmitz et al., 2024), and save it in your R projectsâ€™ root directory (where .Rproj file is located).\n\n# BriefRC Task - Modified functions\n\n# How to use this file:\n# Call this file from your CI generation script (use: source(\"rcicr_briefrc_mod.r\"))\n\n# Conditional installing of required packages\nif(!require(devtools)) install.packages(\"devtools\"); library(devtools)\nif(!require(rcicr)) devtools::install_github('rdotsch/rcicr'); library(rcicr)\n\n# Modified functions below\n\n#' @title Matrix to image\n#' @description Generalized version of \\code{magick::image_read()} which also\n#'  allows to read numerical objects.\n#' @param img Image (e.g., image path) to be read, or numerical object (e.g., matrix, array)\n#' see \\code{magick::image_read()} for more information (equivalent to `path` argument).\n#' @param alpha (optional) Scalar indicating the trasnparency of the (alpha). If the object already\n#' contains an alpha, than it will not be overwritten. Default is `1`.\n#' @param density see \\code{magick::image_read()}\n#' @param depth see \\code{magick::image_read()}\n#' @param strip see \\code{magick::image_read()}\n#' @examples NULL\n#' @export mat2img\nmat2img &lt;- function(img, density = NULL, depth = NULL, strip = FALSE, alpha = 1) {\n  # If img is a path, try to read the image\n  if (is.character(img)) {\n    if (grepl('png|PNG', img)) {\n      img &lt;- png::readPNG(img)\n    } else if (grepl('jpeg|JPEG|jpg|JPG', img)) {\n      img &lt;- jpeg::readJPEG(img)\n    } else {\n      stop(\"Image format not supported\")\n    }\n  }\n  \n  imgDim &lt;- dim(img)\n  alphaMatrix &lt;- matrix(alpha, imgDim[1], imgDim[2])\n  \n  # If img is a numerical object, transform it into an array which can be read by image_read()\n  if (length(imgDim) &lt; 3) {\n    img &lt;- simplify2array(list(img, img, img, alphaMatrix))\n  } else if (imgDim[3] == 3) {\n    img &lt;- simplify2array(list(img, alphaMatrix))\n  }\n  \n  magick::image_read(img, density = density, depth = depth, strip = strip)\n}\n\n#' @title Read and scale base image\n#' @description Read and scale the base image\n#' @param baseImgPath String specifying path to the base image.\n#' @return Vector of pixels from the base image.\n#' @examples NULL\n#' @export readBaseImg\nreadBaseImg &lt;- function(baseImgPath, maxContrast = TRUE) {\n  # Read image\n  if (grepl('png|PNG', baseImgPath)) {\n    baseImg &lt;- png::readPNG(baseImgPath)\n  } else if (grepl('jpeg|JPEG|jpg|JPG', baseImgPath)) {\n    baseImg &lt;- jpeg::readJPEG(baseImgPath)\n  } else {\n    stop(\"Base image format not supported\")\n  }\n  \n  # Ensure there is only 2 dimensions\n  if (length(dim(baseImg)) == 3) baseImg &lt;- baseImg[, , 1]\n  \n  # Maximize base image contrast\n  if (maxContrast) baseImg &lt;- (baseImg - min(baseImg))/(max(baseImg) - min(baseImg))\n  \n  return( as.vector(baseImg) )\n}\n\n#' @title Generate (and scale) mask from responses\n#' @description Generate (and scale) mask from responses.\n#' @param response Numerical vector specifying the reponses.\n#' @param stim Numerical vector specifying the stimuli number.\n#' @param noiseMatrix Matrix of noise pattern as generated with\n#'   \\code{noiseMatrix &lt;- rcirc::generateStimuli2IFC(..., return_as_dataframe = TRUE)}.\n#' @param baseImg Numerical vector containing the baseImg image or string pointing to the baseImg\n#'   image file. If baseImg is a string, then the baseImg image must in .png or .jpeg.\n#' @param scaling String|Scalar|NULL specifying the scaling method. `\"matched\"` is the default method.\n#'   If a scalar is provided (e.g. 5) than the `\"constant\"` method will be applied.\n#'   If `NULL` no scaling is applied.\n#' @return List with the (un)scaled Noise mask (\\code{$mask}) and the base image as a vector\n#'   (\\code{$baseImgVect}).\n#' @examples NULL\n#' @export genMask\ngenMask &lt;- function(response, stim, noiseMatrix, baseImg, scaling = \"matched\") {\n  # Generate mask\n  X  &lt;- data.table::data.table(response = response, stim = stim)\n  X  &lt;- X[, .(response = mean(response)), stim]\n  mask &lt;- (noiseMatrix[, X$stim] %*% X$response) / length(X$response)\n  \n  # Read base image\n  if (is.character(baseImg)) baseImg &lt;- readBaseImg(baseImg)\n  \n  # Scale mask\n  if (scaling == \"matched\") {\n    scaledMask &lt;- min(baseImg)+((max(baseImg)-min(baseImg))*(mask-min(mask))/(max(mask)-min(mask)))\n  } else if (is.numeric(scaling)) { # constant scaling\n    scaledMask &lt;- (mask + scaling)/(2 * scaling)\n    if (max(scaledMask) &gt; 1 | min(scaledMask) &lt; -1) warning(\"Constant is too low! Adjust.\")\n  } else if (is.null(scaling)) { # No scaling\n    scaledMask &lt;- mask\n  }\n  \n  return(list(mask = scaledMask, baseImgVect = baseImg))\n}\n\n#' @title Generate Classification Image (CI)\n#' @description Generate the combinaed of the noise mask (mask) and the base image.\n#' @inheritParams genMask\n#' @param outputPath String specifying the file path of the ouput CI. Default is `\"combined.png\"`.\n#' @return NULL\n#' @examples NULL\n#' @export genCI\ngenCI &lt;- function(response, stim, noiseMatrix, baseImg, scaling = \"matched\",\n                  outputPath = \"combined.png\") {\n  # Generate mask\n  M &lt;- genMask(response, stim, noiseMatrix, baseImg, scaling)\n  mask &lt;- M$mask\n  baseImgVect &lt;- M$baseImgVect\n  \n  # Write and save combined image\n  combined &lt;- (baseImgVect + mask) / 2\n  combined &lt;- matrix(combined, nrow = 512)\n  png::writePNG(combined, outputPath)\n  \n  # Return file path\n  invisible(outputPath)\n}\n\n\n#' @title Get face region\n#' @description Returns a logical vector with the face region\n#' @param imgPath String specifying path to the base image.\n#' @param xpos Numeric specifiying the X position (relative to the center).\n#' @param ypos Numeric specifiying the Y position (relative to the center).\n#' @param faceWidth Numeric specifiying the width of the face region.\n#' @param faceHeight Numeric specifiying the height of the face region.\n#' @param preview Numeric specifiying the height of the face region.\n#' @param writeImgTo String specifying where and if the output image should be saved. Default is\n#'   NULL, meaning that the image will not be saved.\n#' @return Logical vector specifying the location of the face region\n#' @examples NULL\n#' @export getFaceRegion\ngetFaceRegion &lt;- function(imgPath,\n                          xpos = 0, ypos = 0, faceWidth = 1.4, faceHeight = 1.8,\n                          preview = TRUE, writeImgTo = NULL) {\n  # Read image and convert to matrix\n  face &lt;- readBaseImg(imgPath)\n  faceLength &lt;- sqrt(length(face)) # must be squared\n  face &lt;- matrix(face, ncol = faceLength)\n  \n  # Define face region: https://dahtah.github.io/imager/gimptools.html\n  Xcc &lt;- function(im) imager::Xc(im) - imager::width(im)/2  + ypos\n  Ycc &lt;- function(im) imager::Yc(im) - imager::height(im)/2 + xpos\n  NN &lt;- imager::as.cimg( matrix(1:faceLength^2, nrow = faceLength) )\n  faceRegion &lt;- (Xcc(NN)/faceHeight)^2 + (Ycc(NN)/faceWidth)^2 &lt; 100^2\n  faceRegion &lt;- as.vector(faceRegion)\n  \n  # Preview in Viewer\n  if (preview) {\n    alphaMask &lt;- matrix(1, faceLength, faceLength)\n    alphaMask[!faceRegion] &lt;- 0.6\n    previewFace &lt;- abind::abind(face, alphaMask, along = 3)\n    previewFacePath &lt;- tempfile(fileext = \".png\")\n    png::writePNG(previewFace, previewFacePath)\n    invisible(capture.output(print(magick::image_read(previewFacePath))))\n  }\n  # Write face\n  if (!is.null(writeImgTo)) {\n    alphaMask &lt;- matrix(1, faceLength, faceLength)\n    alphaMask[!faceRegion] &lt;- 0\n    printedFace &lt;- abind::abind(face, alphaMask, along = 3)\n    png::writePNG(printedFace, writeImgTo)\n  }\n  \n  invisible(faceRegion)\n}\n\n\n\nScript to generate Individual CI\nHere is how you can generate the individual CI from the Brief RC task data:\n\n# Clear environment\nrm(list=ls())\n\n# Libraries\nlibrary(tidyverse)\nlibrary(here)\nlibrary(purrr)\nlibrary(readr)\nlibrary(vroom)\nlibrary(data.table)\nlibrary(png)\n\n# Modified rcicr function\n\n# Make sure this file is in the same directory as the current file (both should be by default at the RProj root dir)\nsource(\"rcicr_briefrc_mod.r\")\n\n# Load data\n\n# LOAD A SINGLE PARTICIPANT'S DATA FILE\nrawdata &lt;- vroom::vroom(here::here(\"data\", \"brief_rc_data_Rz2aqjzk31.csv\")) # Replace CSV file with your own file\n\n# Data Processing\n\n# Keep only relevant variables for CI generation\ninput_df &lt;- rawdata %&gt;% \n  dplyr::select(participant_number, trial, image_number, image_type, selected_image) %&gt;% \n  mutate(response = image_type) # clarifies that the participant response is the same as teh selected image_type\n\n# Inspect data set\ninput_df %&gt;% names\n\n# Rename participant_number as id and change its type to factor\nrc_brief_df &lt;- input_df %&gt;% mutate(id = participant_number %&gt;% as.factor)\n\n# Generate CIs\n\n# Parameters\nbasePath &lt;- \"FMNES.jpg\" # Base face image, loacted in root directory\nnoisemat &lt;- as.matrix( fread(\"noise_matrix.txt\") ) # Load the noise matrix created at the time of stimuli generation\n\n# Individual CIs\n\n# Creating sub directory to store individual CIs\nindcis_dir &lt;- \"ind_cis\"\nif (!dir.exists(indcis_dir)) {\n dir.create(indcis_dir)\n message(sprintf(\"Directory '%s' created.\", indcis_dir))\n} else {\n message(sprintf(\"Directory '%s' already exists.\", indcis_dir))\n}\n\n# convert data frame to data.table object (data.table handles larger data better and more efficiently)\nrc_brief_dt &lt;- rc_brief_df %&gt;% as.data.table()\n\n# Generate individual CI(s)\nrc_brief_dt[, genCI(\n  outputPath  = paste0(\"ind_cis/ind_\", id, \".png\")[1],\n  stim        = image_number,\n  response    = response,\n  baseImg     = basePath,\n  scaling     = \"matched\", # this parameter might have to be tweaked by trial and error if you use different base images\n  noiseMatrix = noisemat\n), id]\n\nYou can find your individual CI result in the â€ ind_cisâ€ folder. Itâ€™s file name includes the participant number associated with it.\n\n\n\n\n\n\nReferences\n\nLundqvist, D., Flykt, A., & Ã–hman, A. (1998). Karolinska Directed Emotional Faces (KDEF) [Database record]. APA PsycTests. https://doi.org/10.1037/t27732-000\nSchmitz, M., Rougier, M., & Yzerbyt, V. (2024). Introducing the brief reverse correlation: An improved tool to assess visual representations. European Journal of Social Psychology. Advance Online Publication. https://doi.org/10.1002/ejsp.3100"
  },
  {
    "objectID": "media.html",
    "href": "media.html",
    "title": "Media & Outreach",
    "section": "",
    "text": "BNR Nieuwsradio (Wetenschap Vandaag) | AI & Art\n\nâ€œWhen do we object to AI art? (Kunst gemaakt door AI: wanneer vinden we dat erg?)â€ A discussion with Karlijn M. on the psychological friction between human creativity and algorithmic output. (English interview). Listen here"
  },
  {
    "objectID": "media.html#interviews",
    "href": "media.html#interviews",
    "title": "Media & Outreach",
    "section": "Interviews",
    "text": "Interviews\n\nProlific Research Blog | Trust in AI Faces\n\nâ€œThe label effectâ€ Featured interview discussing my research on how the mere label of â€œAIâ€ (or â€œartificialâ€ or â€œsyntheticâ€) degrades trust in pictures and videos of human faces, and what this means for digital deception. Read article"
  },
  {
    "objectID": "media.html#public-engagement",
    "href": "media.html#public-engagement",
    "title": "Media & Outreach",
    "section": "Public Engagement",
    "text": "Public Engagement\n\nBetweter Science Festival | Live Public Experiment\n\nâ€œImitation Games and AI-generated Art (Kunst Kunstmatig)â€: Designed and executed a public experiment testing if groups of festival visitors could distinguish human art from AI generation. LinkedIn post here\n\nThis experiment simulated the experience of visiting a museum in the near future, where artworks can be human-made or AI-generated. The experiment tested how the confirmation or disconfirmation of our expectations regarding who created an artwork (AI, Human, or Co-creation with AI) impacted how we felt about the artwork after learning how it was created. For example, will you like an artwork more if you think it was made by an AI, but turns out it was made by a human?\n\n\nNote: Part of the visual stimuli were real hybrid human-AI artwork created by Noah Hoeboer\n\n\nOur preliminary results (see images) revealed that when expectations about the creator of the artwork were confirmed (guessing human when it was human), people evaluated human-made art more positively than CO-created or AI-generated art. When expectations were disconfirmed (guess human but it was AI), people evaluated Co-created art more negatively than both AI or Human-generated art. Regardless of these findings, most people preferred to take home a postcard of the human-made artworks."
  },
  {
    "objectID": "publications.html",
    "href": "publications.html",
    "title": "Publications",
    "section": "",
    "text": "2024\nOliveira, M., Brands, J., Mashudi, J., Liefooghe, B., & Hortensius, R. (2024). Perceptions of artificial intelligence systemâ€™s aptitude to judge morality and competence amidst the rise of Chatbots. Cognitive Research: Principles and Implications, 9 (47). https://doi.org/10.1186/s41235-024-00573-7\n\n\n2023\nLiefooghe, B., Oliveira, M. J. B., Leisten, L. M., Hoogers, E., Aarts, H., & Hortensius, R. (2023). Are natural faces merely labelled as artificial trusted less? Collabra: Psychology, 9(1): 73066. doi: https://doi.org/10.1525/collabra.73066\n\n\n2022\n[Preprint] Liefooghe, B., Oliveira, M. J. B., Leisten, L. M., Hoogers, E., Aarts, H., & Hortensius, R. (2022, June 10). Faces Merely Labelled as Artificial are Trusted Less. https://doi.org/10.31234/osf.io/te2ju\nOliveira, M., & Garcia-Marques, T. (2022). The effect of facial occlusion on facial impressions of trustworthiness and dominance. Memory & Cognition, Advance Online Publication. https://doi.org/10.3758/s13421-022-01316-z\nGarcia-Marques, T., Oliveira, M., & Nunes, L. (2022). That person is now with or without a mask: how encoding context modulates identity recognition. Cognitive Research: Principles and Implications, 7(29), 1-17. https://doi.org/10.1186/s41235-022-00379-5\n\n\n2020\nOliveira, M., Garcia-Marques, T., Garcia-Marques, L. , & Dotsch, R. (2020). Good to bad or bad to bad? What is the relationship between valence and the trait content of the Big Two? European Journal of Social Psychology, 50(2), 463-483. https://doi.org/10.1002/ejsp.2618\n\n\n2019\nOliveira, M., Garcia-Marques, T., & Dotsch, R. (2019). Combining traits into a face: A reverse correlation approach. Social Cognition, 37(5), 516-545. doi: 10.1521/soco.2019.37.5.516\nOliveira, M., Garcia-Marques, T., Dotsch, R., & Garcia-Marques, L. (2019). Dominance and competence face to face: Dissociations obtained with a reverse correlation approach. European Journal of Social Psychology, 49(5), 888-902. https://doi.org/10.1002/ejsp.2569\n\n\n2017\nSantos, A., Almeida, F., Palma, T., Oliveira, M., & Garcia-Marques, L. (2017). The cultural stereotype of professional groups: Consensus, accessibility and typicality of stereotypic contents. AnÃ¡lise PsicolÃ³gica, 35, 557-568. https://doi.org/10.14417/ap.1385\n\n\n2016\nRamos, T., Oliveira, M., Santos, A. S., Garcia-Marques, & L. Carneiro, P. (2016). Evaluating young and old faces on social dimensions: Trustworthiness and dominance. PsicolÃ³gica, 37(2), 169-185.\n\n\n2012\nOliveira, M., & Miranda, M. (2012). Paradigma: Teste de associaÃ§Ã£o implÃ­cita [Paradigm: Implicit association test]. LaboratÃ³rio de Psicologia, 10, 235-249. doi: 10.14417/lp.673\n\n\nPh.D.Â Dissertation\nOliveira, M.J.B. (2020). The structuring role of valence in the relationship between and within models of face and trait impressions."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Manuel Oliveira",
    "section": "",
    "text": "My interests and work are best placed at the intersection of Psychology and Artificial Intelligence. Currently, I serve as a researcher and advisor at Eindhoven University of Technology (TU/e), focusing on topics of AI in Education.\nMy background is rooted in Social Cognition. For most of my academic journey, I have researched how humans form impressions about other humans. With the increasing presence of AI in society, my research interests started to drift towards questions related with human-AI interaction and the inopact of AI in society. Some of the questions I have worked on include: how humans perceive and trust non-human/articial entities, how they perceive the capabilities of LLMs to match humanlike abilities to judge human behaviors, how students perceive and engage with AI in high-stakes assessment contexts (and importantly what to do about it?)."
  },
  {
    "objectID": "about.html#experience",
    "href": "about.html#experience",
    "title": "Manuel Oliveira",
    "section": "Experience",
    "text": "Experience\n\nTask force AI in Education | 2024 - Present - Eindhoven University of Technology (IE&IS)\n\n-   In this role, I provide input on the latest relevant research and technological developments, with the aim to inform educational policy makers. Other activities include: writing content for public engagement (newsletter, website), designing and conducting workshops to inform and up-skill relevant stakeholders (faculty, support staff, students, entrepreneurs).\n\nPostdoctoral Researcher | 2024 - Present\n\nEindhoven University of Technology\nMy research focuses on finding novel assessment approaches to capture evidence of learning in written assignments despite the use of Generative AI tools (e.g.Â Oliveira et al., 2025). Other projects I worked in explored the utility of GenAI technology to scaffold teacher feedback to students in CBL contexts (preprint)).\n\nPostdoctoral Researcher & Program Coordinator | 2022 - 2023\n\nUtrecht University, Coordination Human-AI Alliance Program\nManaged inter-institutional grant calls and conducted research on trust in human-AI interaction.\nLecturer & Supervisor: Experimental Methods, AI Research Methods, and MSc Thesis Supervision.\n\nData Modeller & Researcher (R&D) | 2020 - 2022\n\nPhilips (Eindhoven), Digital Cognitive Diagnostics\nDeveloped data-driven models for clinical decision support and digital assessment tools.\n\nTeaching Assistant & Researcher | 2012 - 2019\n\nMultiple universities: ISPA - Instituto UniversitÃ¡rio, University of Lisbon, ISCTE-IUL\nSpecialized in Experimental Design, Statistics, and Social Cognition."
  },
  {
    "objectID": "about.html#education",
    "href": "about.html#education",
    "title": "Manuel Oliveira",
    "section": "Education",
    "text": "Education\nPhD in Social Psychology | 2015 - 2019 - ISPA - Instituto UniversitÃ¡rio (Lisboa, Portugal) & Utrecht University (Utrecht, Netherlands) - Graduated with Honor and Distinction - Dissertation: â€œThe structuring role of valence in the relationship between and within models of face and trait impressionsâ€\nMSc in Social and Organizational Psychology | 2009 - 2011 - ISPA - Instituto UniversitÃ¡rio | Lisbon, Portugal\nBSc in Psychology | 2006 - 2009 - University of Aveiro | Aveiro, Portugal"
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Title\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\nDate\n\n\nTitle\n\n\nAuthor\n\n\nReading Time\n\n\n\n\n\n\nJune 11, 2023\n\n\nUsing Reverse Correlation Methods in Psychology Research: R tutorial and Recommended Practices\n\n\nM. Oliveira\n\n\n18 min\n\n\n\n\nAugust 22, 2022\n\n\nHi, my name isâ€¦\n\n\nM. Oliveira\n\n\n2 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "apps.html#carefully-prompt-your-ai",
    "href": "apps.html#carefully-prompt-your-ai",
    "title": "Reverse Correlation: Sampling Subgroup CIs",
    "section": "",
    "text": "Ethical guidance for GenAI-assisted academic writing\nLaunch the CARefully Prompt Library\nThis interactive prompt library was developed to support PhD students and early-career researchers in navigating the complex ethics of using Generative AI in academic writing activities. This tool was developed for a workshop at the 2025 SEFI Summer School(20-23 May @ Den Bosch, The Netherlands), it operationalizes the recent ethical framework by Cheng et al.Â (2025) to help writers make informed decisions.\n\nEthical tiers: Prompts are classified (Tier 1-3) based on Cheng et al.â€™s taxonomy, helping you distinguish between â€œsafeâ€ tasks (e.g., editing) and those requiring heavy oversight (e.g., ideation).\nOffers a structured starting point: While not an empirically validated tool (feel free to use it in your educational pilot studies though!), this library offers a structured â€œsandboxâ€ of templates and examples. Its goal is to inspire better prompting habits and provide a concrete way to reflect on the ethical weight of different writing tasks.\nWriting goal-oriented : Browse prompts for specific needs like for example Improving Clarity, Drafting Assistance, or Maintaining Authorial Voice."
  }
]