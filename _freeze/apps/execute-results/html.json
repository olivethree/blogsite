{
  "hash": "7c3a9d1500f02574a7dd84f44440aa92",
  "result": {
    "markdown": "---\ntitle: \"Tools\"\neditor: visual\n---\n\n\n# [Brief Reverse Correlation Task](https://github.com/olivethree/briefRC)\n\nRun the Brief Reverse Correlation Task locally in your browser (no internet required).\n\nDemo version of full web task here: <https://olivethree.github.io/briefrc12online/>\n\nIn this demo you can see how the full task works and obtain actual results you can use for learning. Using the target category \"Star Wars Fan\" for the sake of demonstartion. Prefer Star Trek? Let me know and I'll make that version! :)\n\n\\\n\\## Generating stimuli for the task\n\nSelect a base face image. In this example, I use the average of the average male and average female faces, facing forward and with neutral expression, from the Karolinska Face Database (Lundqvist, Flykt, & Öhman, 1998).\n\nNext you, you need to decide how many trials you need for your task. In a traditional 2-image forced-choice Reverse Correlation task, we need a pair of images per trial. Therefore, the total images you need to generate as stimuli for the task = \\[number of trials you want\\] x \\[number of images per trial\\]. For the case of 2 images per trial, this would be 600 images for 300 trials.\n\nHowever, the benefit of using Brief RC is so you can present more images per trial. In this tutorial I focus on the version with 12 images per trial. Twelve images per trial is, in my opinion, a reasonable amount to present in most devices' screens, while keeping a manageable informational load for the participants.\n\nIn the Brief RC 12 version, 12 images are presented in each trial. These are not random images from the pool of stimuli. Instead, these are actually 6 pairs of images, where each pair is associated with a specific number (assigned during the noise generation procedure) and includes an original noise patch (labelled as *ori*) and its inverted noise (labelled as *inv*).\n\nIn this case, I will follow the trials used by the Brief RC's authors (Schmitz et al., 2024) for the Brief RC 12 task:\n\n**Total trials:** 60\n\n**Images per trial:** 12 (6 pairs of ori-inv images)\n\n**Total images to generate** = 720\n\nFinally, the generated images are resized for proper fit within the screen area. Image stimuli in the RC task (used in psychology research) are frequently 512x512 px, or 256x256, or 128x128. Here, we will use 128x128 as the final output size for the stimulus images. Importantly, your base face image will need to be resized to these dimensions as well, and make sure this does not stretch or alter the original base face image in any way (unless you have any reasons to distort the natural configuration of a face).\n\n### R script to generate stimuli\n\nNote: R scripts are adapted from the original materials shared by Schmitz et al. (2024).\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Required packages\nlibrary(tidyverse)\nlibrary(data.table)\nlibrary(magick)\nlibrary(magrittr)\n\n# Github packages\nlibrary(devtools)\n# rcicr\nif(!require(rcicr)) devtools::install_github('rdotsch/rcicr'); library(rcicr)\n\n# Parameters\n{# Seed for reproducibility}\ngen_seed   <- 1984\n\n# Define how many task trials you want\nnr_task_trials <- 60\n\n# Number of image pairs (ori + inv)\nnr_rc_trials <- nr_task_trials * 6\n\ncat(\"Setting script to generate stimuli for a total of: \", nr_task_trials, \"trials (Brief RC 12 version).\\n\")\ncat(\"This means you will have a total of \", nr_rc_trials, \"image pairs, or \", nr_rc_trials*2, \"total amount of images.\")\n\n# Path for base face image file (average of the average male and average female face of the Karolinska face database, neutral expression, frontal pose)\n\npath_baseface <- \"FMNES.jpg\" # Replace with your own base image\n\n# Output directory to store the stimuli images\noutput_path  <- \"img/\"\n\n# Stimuli generation\nnoise_matrix <- rcicr::generateStimuli2IFC(\n  base_face_files     = list('avg' = path_baseface),\n  n_trials            = nr_rc_trials,\n  seed                = gen_seed,\n  save_as_png         = TRUE,\n  stimulus_path       = output_path,\n  return_as_dataframe = TRUE,\n  save_rdata          = FALSE\n)\n\n# Noise matrix\n\n# Convert noise matrix to a data.table object\nnoise_matrix <- data.table(noise_matrix)\n\n# save it as txt file\nfwrite(noise_matrix, file = \"noise_matrix.txt\", sep = \" \", row.names = FALSE, col.names = FALSE)\n\n# Rename image files\n\n# Ori files\nlist.files(path = output_path, pattern = \"ori.png\", full.names = TRUE) %>%\n  set_names(paste0(output_path, \"faceOri\", seq_along(.), \".png\")) %>%\n  walk2(., names(.), file.rename)\n\n# Inv files\nlist.files(path = output_path, pattern = \"inv.png\", full.names = TRUE) %>%\n  set_names(paste0(output_path, \"faceInv\", seq_along(.), \".png\")) %>%\n  walk2(., names(.), file.rename)\n\n# Resize image files for brief RC task\n\n# Define image size (original brief RC paper used 150 x 150 px)\nnew_size <- \"150\"\n\n# Create the 'resized' directory if it doesn't exist\ndir.create(file.path(output_path, \"resized\"), showWarnings = FALSE)\n\n# Process and move images to 'resized' \nlist.files(output_path, pattern = \"face\", full.names = TRUE) %>%\n  walk(function(img_path) {\n    outpath <- gsub(\".png\", \"s.png\", img_path)\n    resized_path <- file.path(output_path, \"resized\", basename(outpath))\n    image_read(img_path) %>%\n      image_scale(new_size) %>%\n      image_write(path = resized_path, format = \"png\")\n  })\n```\n:::\n\n\nOnce the generation is completed, you can find the image stimulus set in \"img/resized\". All you need to do next is to copy paste these images to the appropriate experiment folder containing the images (see below).\n\nImportantly, this script generates the noise matrix file (very large file), that contains the information you need to compute the classification images after you run the experiment.\n\n## Setting up the experiment\n\nThese instructions outline how to set up and run the Brief Reverse Correlation task developed by Schmitz et al. (2024) on your local machine.\n\n#### **1. Create a project folder:**\n\nBegin by creating a new folder on your computer to house the task files. Choose a descriptive name for this folder (e.g., `Brief_RC_Task`, `BRC_Experiment`, or similar). This will help keep your files organized.\n\n#### **2. Download files from GitHub:**\n\nNavigate to the GitHub repository: <https://github.com/olivethree/briefRC>. You will need to download the following:\n\n-   **HTML File:** Download the HTML file that contains the application code.\n\n-   **`images` folder:** Download the entire `images` folder, including all its contents (the image files used in the task). *It is crucial to maintain the folder structure; do not just download the images individually.*\n\n#### **3. Project structure:**\n\nPlace the downloaded HTML file and the `images` folder (with its contents) directly into the project folder you created in Step 1. The structure should look like this:\n\n```         \nBrief_RC_Task/       (Your project folder)\n├── index.html      (Example HTML filename, but can be for example demo_briefrc_12.ENG.html)\n└── images/         (The images folder, do not change the name of this folder)\n    ├── *.png       (filenames follow a strict format like for example faceOri<number>.png)\n    ├── *.png\n    └── ...          (Other image files)\n```\n\n#### **4. Adjust experiment content to your needs:**\n\nYou can adjust the informed consent, task instructions, and trial instruction (very important as this is the target category you are interested in, e.g. Select the face that looks like \\<YOUR_CATEGORY_OF_INTEREST\\>).\n\nTo adjust these instructions, you can simply edit the content of the HTML file by opening it in your favorite IDE (e.g. [Visual Code](https://code.visualstudio.com/), [Notepad++,](https://notepad-plus-plus.org/) Xcode, etc.) or text editor, and look for the text. In case you get lost, you can ask ChatGPT or similar chatbots for guidance on where to find this content (or even change it in a more efficient way through prompt engineering if you know what you're doing...just remember to be critical of the output of generative AI, always verify!).\n\n#### **5. Run the experiment**\n\nOpen the HTML file (e.g., `index.html`) in your preferred web browser (e.g., Chrome, Firefox, Safari, Edge). The experiment should now load and be ready to use.\n\n#### **6. Results:**\n\nUpon completion of a session, the results will be automatically saved to your browser's default downloads folder (typically named \"Downloads\").\n\n## Generating Individual Classification Images from Brief RC task data\n\nIn this section I provide an example of how to generate an individual classification image based on the data output of the Brief RC task. I will use the output format of the web version of the task I describe above (<https://olivethree.github.io/briefrc12online/>).\n\n**Note:** *The code below only covers the generation of individual classification images. You can also generate Group CIs using the data from multiple participants, and loading it into R by combining into a single dataset. The data required for Group CI generation includes an additional column (string/factor) specifying target judgment condition, such as Star Wars Fan or Trustworthy. This additional column can then be used to compute the CI with the aggregated condition level data.*\n\nThe output from the web version of the task I indicate above includes the following data:\n\n+---------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n| Column              | Description                                                                                                                                                                                                                      |\n+=====================+==================================================================================================================================================================================================================================+\n| participant_number  | \\[Essential\\] Identifier for each participant, provided by the user/experimenter in the first screen.                                                                                                                            |\n+---------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n| response_id         | \\[Essential\\] Unique identifier for each response. Can be used together with (or instead of) participant_number, point being you need a unique identifier for each participant.                                                  |\n+---------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n| ic_agree            | Whether the participant agreed to informed consent (True/False)                                                                                                                                                                  |\n+---------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n| trial               | \\[Essential\\] The trial number in the experiment. In this case, a total of 60 trials (per participant).                                                                                                                          |\n+---------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n| selected_image      | File name of the specific image chosen by the participant during the trial                                                                                                                                                       |\n+---------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n| image_number        | \\[Essential\\] A numeric identifier for the selected image. This is crucial information, connected to the noise matrix text file created during the generation of the CIs for the task (see Script 1)                             |\n+---------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n| image_type          | Indicates the type of image selected. Therefore, this is also the RESPONSE, reflecting the participant's decision.                                                                                                               |\n|                     |                                                                                                                                                                                                                                  |\n|                     | In a trial there are several pairs of images. Each pair is associated with a specific number (i.e., image_number). One of the images in the pair is original noise (coded as +1), the other is the inverted noise (coded as -1). |\n+---------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n| timestamp           | Timestamp of trial start.                                                                                                                                                                                                        |\n+---------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n| image_1 to image_12 | The 12 file names of the images displayed during the trial. Useful to verify if the 12 images correspond to six pairs of ori-inv images.                                                                                         |\n+---------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n| age                 | Participant's age.                                                                                                                                                                                                               |\n+---------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n| gender              | Participant's gender.                                                                                                                                                                                                            |\n+---------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n\nNot that if your data looks different, you need to tweak the code to adapt it to your case (the moidified function should always stay the same though).\n\n## Modified function to generate BriefRC CIs\n\nCreate a .R file with this content, and save it in your R projects' root directory (where .Rproj file is located).\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# BriefRC Task - Modified functions\n\n# How to use this file:\n# Call this file from your CI generation script (use: source(\"rcicr_briefrc_mod.r\"))\n\n# Conditional installing of required packages\nif(!require(devtools)) install.packages(\"devtools\"); library(devtools)\nif(!require(rcicr)) devtools::install_github('rdotsch/rcicr'); library(rcicr)\n\n# Modified functions below\n\n#' @title Matrix to image\n#' @description Generalized version of \\code{magick::image_read()} which also\n#'  allows to read numerical objects.\n#' @param img Image (e.g., image path) to be read, or numerical object (e.g., matrix, array)\n#' see \\code{magick::image_read()} for more information (equivalent to `path` argument).\n#' @param alpha (optional) Scalar indicating the trasnparency of the (alpha). If the object already\n#' contains an alpha, than it will not be overwritten. Default is `1`.\n#' @param density see \\code{magick::image_read()}\n#' @param depth see \\code{magick::image_read()}\n#' @param strip see \\code{magick::image_read()}\n#' @examples NULL\n#' @export mat2img\nmat2img <- function(img, density = NULL, depth = NULL, strip = FALSE, alpha = 1) {\n  # If img is a path, try to read the image\n  if (is.character(img)) {\n    if (grepl('png|PNG', img)) {\n      img <- png::readPNG(img)\n    } else if (grepl('jpeg|JPEG|jpg|JPG', img)) {\n      img <- jpeg::readJPEG(img)\n    } else {\n      stop(\"Image format not supported\")\n    }\n  }\n  \n  imgDim <- dim(img)\n  alphaMatrix <- matrix(alpha, imgDim[1], imgDim[2])\n  \n  # If img is a numerical object, transform it into an array which can be read by image_read()\n  if (length(imgDim) < 3) {\n    img <- simplify2array(list(img, img, img, alphaMatrix))\n  } else if (imgDim[3] == 3) {\n    img <- simplify2array(list(img, alphaMatrix))\n  }\n  \n  magick::image_read(img, density = density, depth = depth, strip = strip)\n}\n\n#' @title Read and scale base image\n#' @description Read and scale the base image\n#' @param baseImgPath String specifying path to the base image.\n#' @return Vector of pixels from the base image.\n#' @examples NULL\n#' @export readBaseImg\nreadBaseImg <- function(baseImgPath, maxContrast = TRUE) {\n  # Read image\n  if (grepl('png|PNG', baseImgPath)) {\n    baseImg <- png::readPNG(baseImgPath)\n  } else if (grepl('jpeg|JPEG|jpg|JPG', baseImgPath)) {\n    baseImg <- jpeg::readJPEG(baseImgPath)\n  } else {\n    stop(\"Base image format not supported\")\n  }\n  \n  # Ensure there is only 2 dimensions\n  if (length(dim(baseImg)) == 3) baseImg <- baseImg[, , 1]\n  \n  # Maximize base image contrast\n  if (maxContrast) baseImg <- (baseImg - min(baseImg))/(max(baseImg) - min(baseImg))\n  \n  return( as.vector(baseImg) )\n}\n\n#' @title Generate (and scale) mask from responses\n#' @description Generate (and scale) mask from responses.\n#' @param response Numerical vector specifying the reponses.\n#' @param stim Numerical vector specifying the stimuli number.\n#' @param noiseMatrix Matrix of noise pattern as generated with\n#'   \\code{noiseMatrix <- rcirc::generateStimuli2IFC(..., return_as_dataframe = TRUE)}.\n#' @param baseImg Numerical vector containing the baseImg image or string pointing to the baseImg\n#'   image file. If baseImg is a string, then the baseImg image must in .png or .jpeg.\n#' @param scaling String|Scalar|NULL specifying the scaling method. `\"matched\"` is the default method.\n#'   If a scalar is provided (e.g. 5) than the `\"constant\"` method will be applied.\n#'   If `NULL` no scaling is applied.\n#' @return List with the (un)scaled Noise mask (\\code{$mask}) and the base image as a vector\n#'   (\\code{$baseImgVect}).\n#' @examples NULL\n#' @export genMask\ngenMask <- function(response, stim, noiseMatrix, baseImg, scaling = \"matched\") {\n  # Generate mask\n  X  <- data.table::data.table(response = response, stim = stim)\n  X  <- X[, .(response = mean(response)), stim]\n  mask <- (noiseMatrix[, X$stim] %*% X$response) / length(X$response)\n  \n  # Read base image\n  if (is.character(baseImg)) baseImg <- readBaseImg(baseImg)\n  \n  # Scale mask\n  if (scaling == \"matched\") {\n    scaledMask <- min(baseImg)+((max(baseImg)-min(baseImg))*(mask-min(mask))/(max(mask)-min(mask)))\n  } else if (is.numeric(scaling)) { # constant scaling\n    scaledMask <- (mask + scaling)/(2 * scaling)\n    if (max(scaledMask) > 1 | min(scaledMask) < -1) warning(\"Constant is too low! Adjust.\")\n  } else if (is.null(scaling)) { # No scaling\n    scaledMask <- mask\n  }\n  \n  return(list(mask = scaledMask, baseImgVect = baseImg))\n}\n\n#' @title Generate Classification Image (CI)\n#' @description Generate the combinaed of the noise mask (mask) and the base image.\n#' @inheritParams genMask\n#' @param outputPath String specifying the file path of the ouput CI. Default is `\"combined.png\"`.\n#' @return NULL\n#' @examples NULL\n#' @export genCI\ngenCI <- function(response, stim, noiseMatrix, baseImg, scaling = \"matched\",\n                  outputPath = \"combined.png\") {\n  # Generate mask\n  M <- genMask(response, stim, noiseMatrix, baseImg, scaling)\n  mask <- M$mask\n  baseImgVect <- M$baseImgVect\n  \n  # Write and save combined image\n  combined <- (baseImgVect + mask) / 2\n  combined <- matrix(combined, nrow = 512)\n  png::writePNG(combined, outputPath)\n  \n  # Return file path\n  invisible(outputPath)\n}\n\n\n#' @title Get face region\n#' @description Returns a logical vector with the face region\n#' @param imgPath String specifying path to the base image.\n#' @param xpos Numeric specifiying the X position (relative to the center).\n#' @param ypos Numeric specifiying the Y position (relative to the center).\n#' @param faceWidth Numeric specifiying the width of the face region.\n#' @param faceHeight Numeric specifiying the height of the face region.\n#' @param preview Numeric specifiying the height of the face region.\n#' @param writeImgTo String specifying where and if the output image should be saved. Default is\n#'   NULL, meaning that the image will not be saved.\n#' @return Logical vector specifying the location of the face region\n#' @examples NULL\n#' @export getFaceRegion\ngetFaceRegion <- function(imgPath,\n                          xpos = 0, ypos = 0, faceWidth = 1.4, faceHeight = 1.8,\n                          preview = TRUE, writeImgTo = NULL) {\n  # Read image and convert to matrix\n  face <- readBaseImg(imgPath)\n  faceLength <- sqrt(length(face)) # must be squared\n  face <- matrix(face, ncol = faceLength)\n  \n  # Define face region: https://dahtah.github.io/imager/gimptools.html\n  Xcc <- function(im) imager::Xc(im) - imager::width(im)/2  + ypos\n  Ycc <- function(im) imager::Yc(im) - imager::height(im)/2 + xpos\n  NN <- imager::as.cimg( matrix(1:faceLength^2, nrow = faceLength) )\n  faceRegion <- (Xcc(NN)/faceHeight)^2 + (Ycc(NN)/faceWidth)^2 < 100^2\n  faceRegion <- as.vector(faceRegion)\n  \n  # Preview in Viewer\n  if (preview) {\n    alphaMask <- matrix(1, faceLength, faceLength)\n    alphaMask[!faceRegion] <- 0.6\n    previewFace <- abind::abind(face, alphaMask, along = 3)\n    previewFacePath <- tempfile(fileext = \".png\")\n    png::writePNG(previewFace, previewFacePath)\n    invisible(capture.output(print(magick::image_read(previewFacePath))))\n  }\n  # Write face\n  if (!is.null(writeImgTo)) {\n    alphaMask <- matrix(1, faceLength, faceLength)\n    alphaMask[!faceRegion] <- 0\n    printedFace <- abind::abind(face, alphaMask, along = 3)\n    png::writePNG(printedFace, writeImgTo)\n  }\n  \n  invisible(faceRegion)\n}\n```\n:::\n\n\n## Script to generate Individual CI\n\nHere is how you can generate the individual CI from the Brief RC task data:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Clear environment\nrm(list=ls())\n\n# Libraries\nlibrary(tidyverse)\nlibrary(here)\nlibrary(purrr)\nlibrary(readr)\nlibrary(vroom)\nlibrary(data.table)\nlibrary(png)\n\n# Modified rcicr function\n\n# Make sure this file is in the same directory as the current file (both should be by default at the RProj root dir)\nsource(\"rcicr_briefrc_mod.r\")\n\n# Load data\n\n# LOAD A SINGLE PARTICIPANT'S DATA FILE\nrawdata <- vroom::vroom(here::here(\"data\", \"brief_rc_data_Rz2aqjzk31.csv\")) # Replace CSV file with your own file\n\n# Data Processing\n\n# Keep only relevant variables for CI generation\ninput_df <- rawdata %>% \n  dplyr::select(participant_number, trial, image_number, image_type, selected_image) %>% \n  mutate(response = image_type) # clarifies that the participant response is the same as teh selected image_type\n\n# Inspect data set\ninput_df %>% names\n\n# Rename participant_number as id and change its type to factor\nrc_brief_df <- input_df %>% mutate(id = participant_number %>% as.factor)\n\n# Generate CIs\n\n# Parameters\nbasePath <- \"FMNES.jpg\" # Base face image, loacted in root directory\nnoisemat <- as.matrix( fread(\"noise_matrix.txt\") ) # Load the noise matrix created at the time of stimuli generation\n\n# Individual CIs\n\n# Creating sub directory to store individual CIs\nindcis_dir <- \"ind_cis\"\nif (!dir.exists(indcis_dir)) {\n dir.create(indcis_dir)\n message(sprintf(\"Directory '%s' created.\", indcis_dir))\n} else {\n message(sprintf(\"Directory '%s' already exists.\", indcis_dir))\n}\n\n# convert data frame to data.table object (data.table handles larger data better and more efficiently)\nrc_brief_dt <- rc_brief_df %>% as.data.table()\n\n# Generate individual CI(s)\nrc_brief_dt[, genCI(\n  outputPath  = paste0(\"ind_cis/ind_\", id, \".png\")[1],\n  stim        = image_number,\n  response    = response,\n  baseImg     = basePath,\n  scaling     = \"matched\", # this parameter might have to be tweaked by trial and error if you use different base images\n  noiseMatrix = noisemat\n), id]\n```\n:::\n\n\nYou can find your individual CI result in the \" ind_cis\" folder. It's file name includes the participant number associated with it.\n\n## References\n\n-   Lundqvist, D., Flykt, A., & Öhman, A. (1998). *Karolinska Directed Emotional Faces (KDEF)* \\[Database record\\]. APA PsycTests. [https://doi.org/10.1037/t27732-000](https://psycnet.apa.org/doi/10.1037/t27732-000 \"DOI link\")\n-   Schmitz, M., Rougier, M., & Yzerbyt, V. (2024). Introducing the brief reverse correlation: An improved tool to assess visual representations. *European Journal of Social Psychology*. Advance Online Publication. [**https://doi.org/10.1002/ejsp.3100**](https://doi.org/10.1002/ejsp.3100)\n\n------------------------------------------------------------------------\n\n# [Reverse Correlation: Sampling Subgroup CIs](https://olivethree.shinyapps.io/shinyRC_subgroup_cis/)\n\nThis shiny app facilitates the generation of the so-called 'subgroup' classification images (CIs) for a two-phase reverse correlation methodology.\n\nRead the post at [Blog](https://manueloliveira.nl/posts/p_2/) or [Medium](https://medium.com/@manueljbo/improved-two-phase-reverse-correlation-method-in-psychological-research-overview-and-r-tutorial-dfb66f95553c)\n\nThe use of subgroup CIs is a currently one of the recommended practices for the phase of classification image validation in psychological research involving a two-phase variant of the psychophysical reverse correlation method (e.g. [Dotsch & Todorov, 2012](https://journals.sagepub.com/doi/abs/10.1177/1948550611430272)). In practice, this implies using the data collected during the first phase (reverse correlation task) to generate multiple group-level CIs associated with the same target construct condition (i.e. several 'Trustworthy' subgroup CIs vs. single 'Trustworthy' group CI ). These are then validated through simple ratings on a target category of interest (e.g. how trustworthy? how Portuguese? how attractive? etc.) in the second phase by a entirely new group of raters.\n\nUsing subgroup CIs helps decrease the number of images to rate in the second phase, compared to the alternative (but more time consuming) option of rating all the individual CIs generated by each participant in the first phase. This approach circumvents the issues (e.g. type I error inflation) associated with using a single group CI in the second phase (see [Cone et al., 2021](https://journals.sagepub.com/doi/10.1177/1948550620938616)).\n\nFor more details see: <https://github.com/olivethree/shinyrc_subgroupcis>\n\n------------------------------------------------------------------------\n\n# [Face Masker](https://github.com/olivethree/webcam_face_masker){#face-masker}\n\n**Real-time webcam face masking**\n\nThis application applies a facial mask to any faces captured by the webcam. You can also take snapshots and store them in jpeg format. It was developed during my free time as a hobby project, years before AI chatbots could do this for me in 5 seconds. I leave it up to you to decide how useful this app/code may be to you. The app is quite limited and I stopped working on it a long time ago, but maybe it can inspire you to start something better :)\n\n**Running the app**: For now, the only way to run the Face Masker app is from source code \"\\~/src/**facemasker_main.py**\" script.\n\n[Download Face Masker source code](https://github.com/olivethree/webcam_face_masker/archive/refs/heads/master.zip)\n",
    "supporting": [
      "apps_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}